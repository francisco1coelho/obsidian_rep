[[Background]]
- Introduz o contexto de contact centers: _queues, agents, SLA, AHT, ASA, abandonment rate, occupancy_.
    
- Explica o papel de um **semantic layer** ou modelo de métricas — como o MetricFlow/dbt metrics abstrai cálculos.
    
- Descreve como os dashboards refletem estas métricas em tempo real.
    
- Podes também referir o conceito de **business ontologies** aplicadas à analítica (por exemplo, definir o que é uma “chamada abandonada” semanticamente).

### `**Modelo de Dados em Tempo Real — a “memória viva”**`
`Aqui entra o **schema orientado a eventos**: cada linha é um evento (ex.: CallStarted, CallAnswered, CallCompleted, CallAbandoned).`  
`Cada evento traz metadados (timestamps, agente, fila, duração, resultado, etc.).`  
`Frameworks de streaming como **Apache Flink** ou **Kafka Streams**:`

- `recebem continuamente estes eventos;`
    
- `calculam métricas derivadas (AHT, ASA, SL) em janelas de tempo deslizantes;`
    
- `mantêm **views materializadas** que os dashboards podem ler instantaneamente.`
    

==`Isto garante que os valores mostrados (“tempo médio de espera”, “nível de serviço”) são sempre atualizados em segundos.`==


###  `**Modelo Ontológico — o “significado formal”**`
`A ontologia vai além do schema técnico: define **conceitos e relações** de forma lógica e formal.  
`Por exemplo:`

- `um _Agent_ pertence a uma _Queue_;`
    
- `um _Call_ tem um _State_;`
    
- `_Service Level_ depende do número de _Answered Calls_ em determinado tempo.`
    

==`Num contact center, isto assegura que termos como “ocupação” ou “serviço” significam o mesmo em todo o sistema.`==  
`Mas o mais importante para o teu estágio é que **esta ontologia é o elo entre os dados e o LLM**:`

- `o modelo de linguagem pode interpretar “mostra-me agentes com baixa ocupação” e mapear “baixa ocupação” → métrica _AgentOccupancy_ definida na ontologia/semantic layer.`  
    `Assim, o LLM sabe **qual métrica** usar e **como calculá-la**.`


### `**Ligação dos três níveis**`

- `A **camada semântica** define _o que cada métrica significa_ e _como se calcula_.`
    
- `O **modelo de dados em tempo real** garante _que essas métricas estão atualizadas_.`
    
- `A **ontologia** permite _que o LLM entenda e traduza linguagem natural_ para métricas concretas.`
    

`Juntos, eles formam a base para o sistema que descreves na proposta e no resumo do estágio :`  
`um **motor AI que gera dashboards dinâmicos** em tempo real, de forma consistente e compreensível.`

---
## 2.2 Contact Center Metrics and Data Models

Contact centers generate an immense amount of operational data that reflects the performance of both human agents and automated systems. To ensure efficient management and high-quality service delivery, these organizations rely on a wide range of quantitative indicators, commonly referred to as Key Performance Indicators (KPIs). Real-time analytics systems transform raw call events, queue logs, and customer interactions into meaningful metrics that guide decision-making at multiple organizational levels.

Among the most critical metrics in contact center management are those related to service quality, efficiency, and agent performance. The **Average Handling Time (AHT)** measures the mean duration from the start of a customer interaction until the case is completed, including both talk and wrap-up time. It is one of the primary indicators of efficiency [(Twilio, 2024)]([Queues stats monitoring in Flex | Twilio](https://www.twilio.com/docs/flex/end-user-guide/real-time-reporting/real-time-queues-view?))[Flex Insights Data Model | Twilio](https://www.twilio.com/docs/flex/end-user-guide/insights/data-model?). The **Service Level (SL)** defines the percentage of calls answered within a predefined threshold, typically expressed in contractual KPIs such as “80% of calls answered within 20 seconds.” The **Abandonment Rate** measures the proportion of customers who hang up before reaching an agent. Finally, the **Occupancy Rate** indicates the percentage of logged-in time that agents spend handling interactions, reflecting workload balance and staffing adequacy. These metrics are continuously derived from event streams emitted by telephony servers, CRM platforms, and queue managers. Their real-time computation requires precise timestamping, consistent session identifiers, and reliable event ordering to ensure that aggregates are correctly aligned [(AWS, 2023)]([What is Amazon Kinesis Data Streams? - Amazon Kinesis Data Streams](https://docs.aws.amazon.com/streams/latest/dev/introduction.html)).

To manage the complexity of integrating multiple data sources and definitions, many analytics systems adopt a ==**semantic layer** that standardizes business terminology and ensures consistency across visualizations.== In this layer, entities such as Call, Agent, Queue, and Interaction are formally defined with explicit relationships and calculation rules. This approach enables dashboards and reports to reuse shared logic for computing KPIs without duplicating transformations or introducing inconsistencies. Modern data modeling tools such as **dbt Metrics Layer** and **MetricFlow** implement this semantic abstraction by defining metrics as first-class objects with declarative computation rules. Each metric includes its aggregation method, filtering conditions, and time window, forming a bridge between raw data and business-facing dashboards [(dbt Labs, 2023)]([About MetricFlow | dbt Developer Hub](https://docs.getdbt.com/docs/build/about-metricflow?))[dbt Semantic Layer architecture | dbt Developer Hub](https://docs.getdbt.com/docs/use-dbt-semantic-layer/sl-architecture?). This design also improves maintainability and traceability, allowing analysts and developers to understand how each metric is computed and where it originates within the data pipeline.

The ==**data model** for real-time streams must support both historical and live contexts==. Typically, events are modeled using an event-driven schema, where each record represents a discrete state transition in the interaction lifecycle. Common event types include CallStarted, CallAnswered, CallCompleted, and CallAbandoned. Each event carries a timestamp, queue or agent identifier, and other metadata such as duration, outcome, or waiting time. Streaming frameworks like Apache Flink and Kafka Streams support the creation of derived streams and materialized views that continuously aggregate this information, enabling dashboards to display up-to-date values for metrics such as AHT, ASA, or SL every few seconds [org.apache.kafka.streams.kstream (streams 8.1.0-ce API)](https://docs.confluent.io/platform/current/streams/javadocs/javadoc/org/apache/kafka/streams/kstream/package-summary.html)[Apache Flink Documentation | Apache Flink](https://nightlies.apache.org/flink/flink-docs-master/).

Beyond simple schema definitions, the adoption of an **ontological model** further enhances interoperability and consistency across analytical systems. Ontologies formally define the meaning and relationships between domain concepts — for example, specifying how a Queue relates to an Agent or a Call. In contact center analytics, this ensures that terms like ==“abandoned call” or “service level” are defined uniformly across data sources and visualization layers==. When integrated with natural language interfaces, as explored in this dissertation, such ontologies also allow Large Language Models (LLMs) to interpret user intents more accurately by mapping linguistic expressions (e.g., “show me agents with low occupancy”) to canonical metric definitions. This connection between semantic understanding and metric computation forms the foundation for intelligent dashboard generation.

In summary, contact center analytics depend on a robust combination of real-time metrics, semantic standardization, and ontological modeling. Accurate and timely computation of metrics such as AHT and SL provides immediate operational feedback, while semantic and ontological layers ensure that these computations remain consistent and interpretable across dashboards, data pipelines, and natural language interfaces. These foundations are essential for developing AI-driven analytics systems capable of reasoning over data, generating insights, and dynamically adapting visualizations in real time.