[[Background]]
- Explica o que é _real-time data processing_ (event-driven, streaming-first architecture).
    
- Descreve as camadas típicas:
    
    - **Ingestion** (Kafka, Kinesis, Flink sources)
        
    - **Processing** (Flink, Spark Streaming, Kafka Streams)
        
    - **Serving/Visualization** (Grafana, Superset, Looker, PowerBI, etc.)
        
- Aborda conceitos como _low latency_, _streaming semantics_ (_at least once_, _exactly once_), _windowing_, _stateful processing_.
    
- Podes referir desafios: escalabilidade, tolerância a falhas, sincronização de streams.
    

_(Equivalente à secção “Dependability” no exemplo — contextualiza o domínio técnico.)_


---
### 2.1 Real-Time Analytics Systems
In the era of data-driven decision-making, organizations increasingly depend on real-time analytics to monitor operations, detect anomalies, and respond to events as they occur. Unlike traditional batch processing, which analyzes static datasets collected at fixed intervals, **real-time analytics systems** continuously process and analyze data streams with minimal latency, allowing insights to be generated immediately after events are produced. This capability is particularly crucial in dynamic operational environments such as contact centers, where second-by-second visibility into agent performance, queue times, and service quality directly influences both customer experience and operational efficiency (Stonebraker, 2018).

A **real-time analytics pipeline** generally follows a _streaming-first_ architecture composed of three main layers: data ingestion, data processing, and data serving or visualization. Each layer plays a distinct role in converting raw event streams into actionable insights that can be displayed, queried, or acted upon in near real time.

### Data Ingestion Layer

The **ingestion layer** is responsible for capturing continuous streams of events from multiple heterogeneous sources, such as telephony systems, CRM updates, customer interaction logs, and monitoring APIs. Its main objective is to ensure scalable, fault-tolerant, and ordered data delivery to downstream systems. Modern ingestion platforms such as **Apache Kafka** and **Amazon Kinesis** have become industry standards for handling high-throughput, low-latency streaming workloads [(Apache Software Foundation, 2023; AWS, 2023)]([Apache Kafka](https://kafka.apache.org/)).
[What is Amazon Kinesis Data Streams? - Amazon Kinesis Data Streams](https://docs.aws.amazon.com/streams/latest/dev/introduction.html)
Kafka provides a distributed, partitioned, and replicated log that decouples producers from consumers, enabling asynchronous message delivery and replayability. It supports delivery semantics such as _at most once_, _at least once_, and _exactly once_, with the latter being particularly important for maintaining data consistency in mission-critical systems. Similarly, Amazon Kinesis offers a managed streaming service that automatically scales with workload and provides built-in fault tolerance and persistence through shard replication.

Ingestion systems typically rely on distributed commit logs and offset tracking to guarantee delivery guarantees. They are designed for horizontal scalability and can handle millions of events per second, ensuring that downstream processors can consume data continuously without bottlenecks.

### Data Processing Layer

Once data has been ingested, it must be transformed, aggregated, and analyzed in near real time. This is the role of the **data processing layer**, which applies continuous computation models to incoming streams. Frameworks such as **Apache Flink**, **Apache Spark Structured Streaming**, and **Kafka Streams** are the most widely adopted technologies for this purpose [(Karimov et al., 2018)]([1802.08496](https://arxiv.org/pdf/1802.08496)). These frameworks implement _stateful stream processing_, meaning they can maintain intermediate state across time windows — a key requirement for metrics like _average handling time_ or _queue abandonment rate_, which depend on event sequences and temporal context.

Stream processing frameworks define logical operators such as filters, joins, and aggregations, executed over unbounded event streams. They support **windowing mechanisms** — such as tumbling, sliding, or session windows — which group events based on time intervals or session boundaries. These mechanisms allow continuous recalculation of metrics, for example, “average waiting time in the last five minutes.”

Fault tolerance is achieved through mechanisms like checkpointing and state snapshots, which allow a system to resume from the last consistent state after a failure. This ensures both correctness and resilience under distributed execution. Frameworks like Flink further employ **exactly-once processing guarantees**, ensuring no event duplication or omission even under system restarts (Flink Documentation, 2023).

%% Another essential aspect of the processing layer is **backpressure management** — a technique that dynamically adjusts input flow when downstream components are overloaded, preventing memory exhaustion and maintaining overall system stability. %%

### Data Serving and Visualization Layer

The **serving layer** exposes processed data to end users or downstream systems, typically via dashboards, APIs, or alerting mechanisms. Its primary goal is to deliver low-latency access to computed metrics and visualizations. Depending on the use case, this layer may rely on analytical data stores optimized for real-time querying, such as **ClickHouse**, **Druid**, or **Elasticsearch**, which support time-series indexing and fast aggregations [(Yang et al., 2014)]([yang2014druid.pdf](https://cs-courses.mines.edu/csci598ab/spring2022/assets/papers/yang2014druid.pdf?utm_source=chatgpt.com)).

In operational contexts such as contact centers, the serving layer feeds **real-time dashboards** that display live KPIs, including Service Level (SL), Average Speed of Answer (ASA), or agent occupancy. Visualization tools such as **Grafana**, **Apache Superset**, **Tableau**, or **custom web-based interfaces** are commonly used to render and interact with these metrics [(Grafana Labs, 2024)]([Technical documentation | Grafana Labs](https://grafana.com/docs/)). The serving layer must synchronize with the upstream stream-processing infrastructure and refresh data at frequent intervals to reflect the current operational state.

For real-time dashboards, **data freshness**, **query latency**, and **update frequency** are critical parameters. Many architectures use caching or incremental materialization to achieve sub-second response times while maintaining system scalability.


Dunno if needed
### ~~Design Challenges~~

~~Designing and maintaining a real-time analytics system involves several technical and architectural challenges. **Scalability** is paramount: the system must accommodate fluctuating event rates, especially during peak hours, without degradation in latency or throughput. **Fault tolerance** and **message ordering** are equally vital, as data loss or out-of-order processing can result in misleading metrics and incorrect insights.~~

~~Another challenge is **stream synchronization** — aligning events from multiple sources that may operate under different clocks or latencies. Techniques such as event-time watermarking and timestamp extraction are commonly used to address this issue. **Data quality** and **schema evolution** are additional concerns, as data structures often change over time. To mitigate disruptions, production-grade systems employ schema registries and backward-compatible serialization formats like Avro or Protobuf.~~

~~Finally, **latency optimization** requires balancing consistency with responsiveness. Achieving near-instantaneous analytics often involves in-memory processing, efficient serialization, and adaptive batching. For mission-critical applications such as contact centers, maintaining latency under a few seconds is essential to support timely operational decisions.~~

~~In summary, real-time analytics systems form the technological foundation of modern operational intelligence. Understanding their layered architecture — ingestion, processing, and serving — and the design trade-offs involved is crucial for developing intelligent systems capable of extending these pipelines with advanced components such as Large Language Models (LLMs) for adaptive dashboard generation and semantic reasoning.~~