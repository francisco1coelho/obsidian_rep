[[Background]]
- Explica o que é _real-time data processing_ (event-driven, streaming-first architecture).
    
- Descreve as camadas típicas:
    
    - **Ingestion** (Kafka, Kinesis, Flink sources)
        
    - **Processing** (Flink, Spark Streaming, Kafka Streams)
        
    - **Serving/Visualization** (Grafana, Superset, Looker, PowerBI, etc.)
        
- Aborda conceitos como _low latency_, _streaming semantics_ (_at least once_, _exactly once_), _windowing_, _stateful processing_.
    
- Podes referir desafios: escalabilidade, tolerância a falhas, sincronização de streams.
    

_(Equivalente à secção “Dependability” no exemplo — contextualiza o domínio técnico.)_


---
### 2.1 Real-Time Analytics Systems
In the era of data-driven decision-making, organizations increasingly depend on real-time analytics to monitor operations, detect anomalies, and respond to events as they occur. Unlike traditional batch processing, which analyzes static datasets collected at fixed intervals, **real-time analytics systems** continuously process and analyze data streams with minimal latency, allowing insights to be generated immediately after events are produced. This capability is particularly crucial in dynamic operational environments such as contact centers, where second-by-second visibility into agent performance, queue times, and service quality directly influences both customer experience and operational efficiency (Stonebraker, 2018).

A **real-time analytics pipeline** generally follows a _streaming-first_ architecture composed of three main layers: data ingestion, data processing, and data serving or visualization. Each layer plays a distinct role in converting raw event streams into actionable insights that can be displayed, queried, or acted upon in near real time.

### Data Ingestion Layer

The **ingestion layer** is responsible for capturing continuous streams of events from multiple heterogeneous sources, such as telephony systems, CRM updates, customer interaction logs, and monitoring APIs. Its main objective is to ensure scalable, fault-tolerant, and ordered data delivery to downstream systems. Modern ingestion platforms such as **Apache Kafka** and **Amazon Kinesis** have become industry standards for handling high-throughput, low-latency streaming workloads [(Apache Software Foundation, 2023; AWS, 2023)]([Apache Kafka](https://kafka.apache.org/)).
[What is Amazon Kinesis Data Streams? - Amazon Kinesis Data Streams](https://docs.aws.amazon.com/streams/latest/dev/introduction.html)
Kafka provides a distributed, partitioned, and replicated log that decouples producers from consumers, enabling asynchronous message delivery and replayability. It supports delivery semantics such as _at most once_, _at least once_, and _exactly once_, with the latter being particularly important for maintaining data consistency in mission-critical systems. Similarly, Amazon Kinesis offers a managed streaming service that automatically scales with workload and provides built-in fault tolerance and persistence through shard replication.

Ingestion systems typically rely on distributed commit logs and offset tracking to guarantee delivery guarantees. They are designed for horizontal scalability and can handle millions of events per second, ensuring that downstream processors can consume data continuously without bottlenecks.

### Data Processing Layer

Once data has been ingested, it must be transformed, aggregated, and analyzed in near real time. This is the role of the **data processing layer**, which applies continuous computation models to incoming streams. Frameworks such as **Apache Flink**, **Apache Spark Structured Streaming**, and **Kafka Streams** are the most widely adopted technologies for this purpose [(Karimov et al., 2018)]([1802.08496](https://arxiv.org/pdf/1802.08496)). These frameworks implement _stateful stream processing_, meaning they can maintain intermediate state across time windows — a key requirement for metrics like _average handling time_ or _queue abandonment rate_, which depend on event sequences and temporal context.

Stream processing frameworks define logical operators such as filters, joins, and aggregations, executed over unbounded event streams. They support **windowing mechanisms** — such as tumbling, sliding, or session windows — which group events based on time intervals or session boundaries. These mechanisms allow continuous recalculation of metrics, for example, “average waiting time in the last five minutes.”

Fault tolerance is achieved through mechanisms like checkpointing and state snapshots, which allow a system to resume from the last consistent state after a failure. This ensures both correctness and resilience under distributed execution. Frameworks like Flink further employ **exactly-once processing guarantees**, ensuring no event duplication or omission even under system restarts (Flink Documentation, 2023).

