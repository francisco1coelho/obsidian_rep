[[Background]]
- Explica brevemente o que é um **LLM** (transformer, embeddings, context windows).
    
- Introduz o conceito de **structured output generation** (JSON schema, grammar constraints, function calling).
    
- Mostra como isto se liga ao teu caso: o LLM deve gerar **um JSON de configuração de dashboard**, não apenas texto.
    
- Cita frameworks ou APIs relevantes: OpenAI Function Calling, JSON Schema-guided generation, Microsoft Guidance, Outlines, etc.
    

_(Esta secção faz o papel de “Fault Injection Properties” no exemplo — aprofunda o conceito técnico principal.)_


LLM vai interpretar o input e o output vai ser um JSON por exemplo, JSON esse que depois vai ser interpretado para a geração do gráfico.

---

## 2.3 Large Language Models for Structured Output

The recent advances in **Large Language Models (LLMs)** have transformed how natural language can be interpreted and operationalized in computational systems. LLMs are deep neural networks, that learn to predict the next token in a sequence using self-attention mechanisms. This design allows the model to capture long-range dependencies in text, understand complex linguistic structures, and generate coherent, contextually relevant responses. The strength of transformer-based models lies in their capacity to process tokens in parallel and assign contextual importance weights to each, leading to superior performance in translation, summarization, reasoning, and text generation tasks (OpenAI, 2024).

An LLM operates within a **context window**, which defines the maximum number of tokens it can process at once. Modern models such as GPT-4 and Claude 3 support extended contexts of tens or even hundreds of thousands of tokens, enabling them to reason over large documents or conversation histories. Each token is represented as a **dense embedding vector**, capturing semantic and syntactic relationships within the model’s latent space. These embeddings allow the model to generalize meaning and handle paraphrasing, synonyms, and incomplete user queries—capabilities crucial for understanding natural language prompts in real-world applications such as conversational dashboards.

Traditional LLM outputs are unstructured text, which is adequate for dialogue but insufficient for systems that require precise, machine-readable responses. To address this, recent approaches have introduced mechanisms for **structured output generation**, where the model produces outputs that conform to a predefined schema or format. This ensures consistency, correctness, and interoperability with downstream systems. Common approaches include:

1. **JSON schema–guided generation**, where the model is instructed to output valid JSON conforming to a given schema that defines fields, types, and constraints.
    
2. **Grammar-based decoding**, in which token generation is restricted by a context-free grammar that enforces syntactic validity at inference time.
    
3. **Function calling**, a technique popularized by OpenAI and later adopted by Anthropic and Microsoft, which allows developers to expose a list of callable functions that the model can “invoke” by returning structured arguments in JSON format (OpenAI, 2023).
    

==In the context of this dissertation, structured generation is fundamental because the LLM is not expected to produce free text but rather a **JSON configuration** describing a dashboard==. This configuration includes components such as titles, layouts, widgets, metrics, filters, and thresholds. ==For example, when the user provides a natural language prompt like “show me a queue performance dashboard focusing on wait time and call handling,” the model must generate a syntactically valid JSON object that defines the widgets and metrics corresponding to these concepts. Such output can then be directly interpreted by the existing dashboard rendering engine, effectively turning natural language into a functional interface for data visualization.==

Ensuring **semantic alignment** between user intent and generated configuration requires coupling the LLM with a validation layer that checks the generated JSON against both its schema and the organization’s domain ontology (e.g., what constitutes “queue performance” or “average handle time”). This hybrid design—combining probabilistic reasoning from the LLM with deterministic validation from schema constraints—produces outputs that are both expressive and safe to execute. %%O LLM tem que aprender a sintaxe e a semantica segundo a ontologia definida (geralmente isto está num ficheiro no backend para o LLM aprender)%%

Overall, structured output generation transforms LLMs from conversational agents into **programmable reasoning components** capable of interacting with complex systems safely. For this dissertation, it provides the technical foundation for enabling supervisors to generate, modify, and validate real-time dashboards through natural language, bridging human intent and data visualization logic.

---

### Referências

- Vaswani, A., et al. (2017). _Attention Is All You Need._ Advances in Neural Information Processing Systems (NeurIPS). https://arxiv.org/abs/1706.03762
    
- OpenAI (2023). _Function Calling and Structured Outputs._ [https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)
    
- OpenAI (2024). _JSON Mode and Schema-Guided Generation._ [https://platform.openai.com/docs/guides/structured-outputs](https://platform.openai.com/docs/guides/structured-outputs)
    
- Microsoft (2024). _Guidance: Controlling LLMs with Declarative Prompts._ [https://github.com/microsoft/guidance](https://github.com/microsoft/guidance)
    
- Ollion (2024). _Outlines: Structured Text Generation for LLMs._ [https://github.com/normal-computing/outlines](https://github.com/normal-computing/outlines)
