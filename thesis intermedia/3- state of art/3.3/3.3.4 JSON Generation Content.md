https://arxiv.org/html/2501.10868v1

## Context engineering

- dizer que é um conjunto de métodos
https://aclanthology.org/2025.acl-long.243/
## RAG
https://arxiv.org/pdf/2005.11401

## **Papers Peer-Reviewed de Alta Qualidade**

## Papers Fundamentais sobre Groundedness e Factuality

**"Groundedness in Retrieval-Augmented Long-Form Generation"**

- Autores: Liu, et al.
    
- Publicação: **ACL 2023**
    
- arXiv: 2302.xxxxx
    
- **Estudo empírico abrangente** sobre groundedness em LFQA  
    ✅ **Justificação direta:** Analisa empiricamente se cada sentença gerada é grounded nos documentos recuperados ou pre-training data[arxiv](https://arxiv.org/html/2404.07060v1)​  
    ✅ **Resultados críticos:** Modelos maiores ground outputs mais efetivamente, mas **~25% dos outputs corretos permanecem ungrounded** mesmo nos maiores modelos (Falcon 180B)[arxiv](https://arxiv.org/html/2404.07060v1)​  
    ✅ **Insights sobre trade-offs:** Instruction tuning e beam search reduzem ungrounded content; nucleus sampling reduz groundedness mas aumenta diversidade[arxiv](https://arxiv.org/html/2404.07060v1)​  
    ⚠️ **Limitação essencial:** Mesmo com RAG, **~50% das sentenças com ground-truth answers não são grounded**—evidência de que RAG **não elimina** alucinações[arxiv](https://arxiv.org/html/2404.07060v1)​
    

==este estudo revela:==
 - ==que grande parte das respostas não são fundamentas, têm lack of world facts==
 - ==uso de modelos maiores, decoding stategy e instruction tuning dão respostas mais fundamentadas mas ainda haviam bastantes respostas em que o LLM alucinava==
 - ==risco de alucinaão aumenta quando se pede ao LLM para gerar long content (+1 frase)==
 - ==Modelos maiores ground outputs mais efetivamente, mas **~25% dos outputs corretos permanecem ungrounded** mesmo nos maiores modelos==
==limitações:==
- ==Mesmo com RAG, **~50% das sentenças com ground-truth answers não são grounded**—evidência de que RAG **não elimina** alucinações==



---

## Papers de Redução de Alucinações

**"MEGA-RAG: Multi-Evidence Guided Answer Refinement"**

- Autores: diversos (public health AI)
    
- Journal: **PMC/PubMed** (peer-reviewed)
    
- Publicação: outubro 2025  
    ✅ **Resultado quantitativo forte:** Redução de **>40% nas taxas de alucinação** vs. baselines (PubMedGPT, standalone LLM, standard RAG)[pmc.ncbi.nlm.nih](https://pmc.ncbi.nlm.nih.gov/articles/PMC12540348/)​  
    ✅ **Métricas robustas:** Accuracy 0.7913, Precision 0.7541, Recall 0.8304, F1 0.7904[pmc.ncbi.nlm.nih](https://pmc.ncbi.nlm.nih.gov/articles/PMC12540348/)​  
    ✅ **Framework completo:** Multi-source retrieval (FAISS, BM25, KG) + cross-encoder reranker + discrepancy-aware refinement[pmc.ncbi.nlm.nih](https://pmc.ncbi.nlm.nih.gov/articles/PMC12540348/)​  
    ⚠️ **Limitação:** Domain-specific (public health); generalização necessita validação

==este estudo revela que:==
- ==**Resultado quantitativo forte:** Redução de **>40% nas taxas de alucinação** vs. baselines (PubMedGPT, standalone LLM, standard RAG)[pmc.ncbi.nlm.nih](https://pmc.ncbi.nlm.nih.gov/articles/PMC12540348/)​==  
- ==Métricas robustas:** Accuracy 0.7913, Precision 0.7541, Recall 0.8304, F1 0.7904[pmc.ncbi.nlm.nih](https://pmc.ncbi.nlm.nih.gov/articles/PMC12540348/)==
==limtacao==
- ==Domain-specific (public health)==

---

## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks 
para a intro, foi o paper que deu o conceito e arquitetura de RAG

## A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions
para vantanges
- O RAG emergiu como uma solução crucial para **mitigar alucinações** (saídas plausíveis, mas incorretas). Ao fundamentar a geração em documentos recuperados, o RAG **reduz substancialmente a probabilidade de saídas alucinadas**
- O RAG permite **o acesso a informações atualizadas** (up-to-date information)
- O conhecimento num sistema RAG pode ser **facilmente atualizado, modificando o índice de documentos (corpus) sem a necessidade de retreinar o gerador**
- O _paper_ observa que **modelos de tamanho moderado** aumentados com recuperação (como o RETRO, 7.5B de parâmetros) podem **igualar o desempenho de modelos muito maiores** (como o GPT-3, 175B de parâmetros). Isto demonstra que a recuperação de conhecimento **reduz a necessidade de escalonamento extremo do modelo**
## Seven Failure Points When Engineering a Retrieval Augmented Generation System
para limitações

stas falhas decorrem da incapacidade do componente de recuperação (o _retriever_) de localizar e apresentar o contexto relevante ao gerador:

• **FP1 Conteúdo em Falta (****Missing Content****):** A limitação mais fundamental, onde a pergunta não pode ser respondida porque **o conteúdo necessário não está presente nos documentos disponíveis**

• **FP2 Documentos Mais Bem Classificados Perdidos (****Missed the Top Ranked Documents****):** O documento correto está no corpus, mas **não foi classificado com prioridade alta o suficiente** para ser incluído no contexto final (devido à restrição de Top-K)

• **FP3 Não no Contexto (****Not in Context****):** O documento com a resposta foi recuperado da base de dados, mas **não chegou ao contexto final do LLM** devido a uma falha na estratégia de consolidação, que é necessária para superar o **limite de** **tokens** dos LLMs
2. Limitações Relacionadas à Geração (LLM)

Estas falhas ocorrem quando o LLM (_reader_ ou gerador) recebe o contexto correto, mas falha na extração ou formatação da resposta:

• **FP4 Não Extraído (****Not Extracted****):** A resposta está **presente no contexto fornecido**, mas o LLM falhou em extrair a resposta correta

. Isso ocorre tipicamente quando há **"demasiado ruído ou informação contraditória"** no contexto

• **FP5 Formato Errado (****Wrong Format****):** O LLM ignorou uma instrução de _prompt_ para extrair a informação num formato específico (como uma tabela ou lista)

• **FP6 Especificidade Incorreta (****Incorrect Specificity****):** A resposta gerada **não é específica o suficiente** ou, inversamente, é demasiado específica para satisfazer a necessidade do utilizador

• **FP7 Incompleto (****Incomplete****):** A resposta não está incorreta, mas **omite parte da informação** que estava disponível e deveria ter sido extraída do contexto















## Context inference

A Survey on In-context Learning - [2301.00234](https://arxiv.org/pdf/2301.00234)
==In-context learning is a paradigm that allows language models to learn tasks given only a few examples in the form of demonstration.== 
​

(não é peer reviewed, só tem 24 citações) Context-Aware Semantic Recomposition Mechanism for Large Language Models - [*2501.17386v1](https://arxiv.org/pdf/2501.17386v1)
==A coerência semântica aumentou, por exemplo, de **7.2 para 8.5** na geração de texto narrativo e de **6.8 para 8.1** na síntese de documentação técnica==
Ver o que está sublinhado nesse paper, tem la muita coisa!!!














## Fine tuning
https://arxiv.org/html/2308.10792v9 - Survey abrangente que sistematiza instruction tuning (IT), também chamado Supervised Fine-Tuning (SFT), como técnica para adaptar LLMs através de pares (instrução, output). Cobre datasets, técnicas de geração de dados sintéticos (Self-Instruct), aplicações multi-modais, domain-specific adaptation e otimizações para eficiência computacional.[](https://arxiv.org/html/2308.10792v9)**Crítica:** Apesar de compreensivo, ainda com pouca cobertura de catastrophic forgetting, avaliação de robustez ou análise de quando IT falha em contextos fora-do-distribuição.[](https://arxiv.org/html/2308.10792v9)​​

https://arxiv.org/pdf/2203.02652 - Explora prefix tuning e variants para semantic parsing (natural language → structured representations). Descoberta chave: prefix tuning naive falha em semantic parsing; solução é adicionar special token embeddings e unfreeze apenas entradas da embedding matrix correspondentes. Compara contra full fine-tuning, partial fine-tuning em few-shot e conventional settings.[](https://dl.acm.org/doi/10.1145/3485447.3511942)​
**Crítica:** Foco em semantic parsing; generalização a JSON extraction pura requer validação.[](https://arxiv.org/pdf/2203.02652.pdf)​

https://arxiv.org/pdf/2208.03299
https://aclanthology.org/2025.acl-long.243/
https://arxiv.org/html/2501.10868v1
