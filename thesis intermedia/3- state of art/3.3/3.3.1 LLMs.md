	# **3.1 Fundamentos e Tecnologias Relacionadas**
## ==há algumas referências em falta==
A evolução recente dos **Modelos de Linguagem de Grande Escala (LLMs)** redefiniu a forma como sistemas computacionais interpretam e produzem linguagem natural. Estes modelos tornaram-se o núcleo de múltiplas soluções de inteligência artificial, permitindo transformar instruções textuais em representações estruturadas e executáveis. No contexto mais amplo dos sistemas de análise de dados, os LLMs representam a camada de **interpretação semântica** que viabiliza a tradução de comandos expressos em linguagem natural para formatos formais — como JSON ou SQL.

Assim, esta secção tem como objetivo **contextualizar o panorama tecnológico dos LLMs**, analisando a evolução arquitetónica, a diversidade de abordagens e os principais indicadores de desempenho. 

---

## **Panorama Atual dos LLMs com API Pública**

O ecossistema de LLMs caracteriza-se por uma forte **diversificação de arquiteturas**, suportada por rápidas iterações de versões e um equilíbrio cada vez mais visível entre **modelos open-source** e **modelos proprietários**. As famílias **Llama 4**, **Qwen3** e **Granite 3.2** (de código aberto) e os modelos **GPT-5**, **Gemini 2.5** e **Claude 4.5** (comercialmente licenciados) representam os eixos principais de inovação.
As métricas comparativas aqui apresentadas foram recolhidas a partir de **model cards oficiais**, **leaderboards independentes** (SWE-bench Verified, AIME 2025, LiveCodeBench) e **documentação pública das APIs**, garantindo uma visão equilibrada entre dados auto-relatados e resultados verificados de forma independente.


---

### **Modelos Proprietários**

# chatG

O **GPT-5**, desenvolvido pela **OpenAI** e anunciado oficialmente em **6 de agosto de 2025**, representa a mais recente iteração da família _Generative Pre-trained Transformer_, consolidando-se como **referência no domínio do raciocínio geral e da inferência multimodal avançada** [3]. A arquitetura do modelo introduz uma **integração aprimorada entre raciocínio simbólico e estatístico**, reforçada por um mecanismo de **planeamento interno de cadeia de pensamento (_internal reasoning traces_)**, destinado a aumentar a coerência de respostas em tarefas complexas.

O GPT-5 está disponível em **três variantes dimensionais** — **GPT-5**, **GPT-5 mini** e **GPT-5 nano** — concebidas para diferentes escalas de aplicação e restrições de custo. Segundo a documentação técnica da OpenAI, o modelo suporta uma **janela de contexto máxima de 400 000 tokens**, dos quais cerca de **272 000 tokens correspondem a entrada** e **128 000 tokens a saída** [13]. Esta expansão contextual reflete a aposta da OpenAI em **memória conversacional de longo prazo** e **análise de contexto estendido**, aproximando-o dos limites de raciocínio de modelos como o _Llama 4 Scout_ e o _Gemini 2.5 Pro_.

Nos principais _benchmarks_, o GPT-5 mantém **desempenho de topo**: atinge **94,3 % no AIME 2025**, demonstrando forte capacidade de **raciocínio matemático** [11], e **65,0 % no SWE-bench Verified**, no domínio da **engenharia de software** [15]. De acordo com a **OpenAI**, avaliações internas reportam valores ligeiramente superiores — **94,6 % no AIME 2025** e **74,9 % no SWE-bench** [3] —, discrepância atribuída a diferenças metodológicas entre protocolos independentes e internos. Em termos qualitativos, a empresa destaca a **consistência sintática e semântica** do modelo em tarefas de **_function calling_**, particularmente na geração estruturada de formatos como **JSON** [3].

No plano económico, o GPT-5 apresenta um **custo médio oficial de 1,25 USD por milhão de tokens de entrada**, **10,00 USD por milhão de tokens de saída** e **0,125 USD por milhão de tokens em cache** [10]. Embora competitivo face a modelos de dimensão equivalente, este custo pode **aumentar consideravelmente em ambientes produtivos**, devido a fatores como **latência, throughput, estratégias de cache e sobrecarga de faturação**. Assim, a sua **viabilidade operacional** depende fortemente do perfil de utilização e da política de otimização de _prompting_ adotada.

Conclui-se, portanto, que o **GPT-5** se distingue pela **elevada capacidade de raciocínio, robustez sintática e consistência contextual**, embora apresente **custos de operação superiores** aos modelos _open-source_ equivalentes. A sua arquitetura evidencia o **amadurecimento da linha GPT**, consolidando-a como **referência de desempenho e estabilidade** no panorama dos LLMs de 2025.****


# gemini

O **Gemini 2.5**, desenvolvido pela **Google DeepMind**, representa a mais recente geração da linha **Gemini** de **modelos multimodais de grande escala**, combinando texto, imagem, áudio e código num único sistema de inferência [4]. Disponível em **duas variantes principais** — **Gemini 2.5 Pro** (lançado em **junho de 2025**) e **Gemini 2.5 Flash** (setembro de 2025) —, a série reforça o compromisso da Google com a integração de **raciocínio multimodal, eficiência computacional e escalabilidade operacional**.

A versão **Gemini 2.5 Pro** introduz o modo **_Deep Think_**, concebido para **aumentar a profundidade de raciocínio matemático e multimodal**, permitindo **cadeias de inferência mais longas e explicáveis**. Segundo dados oficiais da **Google DeepMind**, o modelo alcança **88 % no _benchmark AIME 2025_** [4], demonstrando um avanço substancial face à geração 2. Em contrapartida, **avaliações independentes** reportadas por plataformas de _benchmarking_ como o **ArtificialAnalysis** indicam valores ligeiramente inferiores, em torno de **87,7 %**, diferença atribuída às **variações metodológicas nos protocolos de teste** [11].

Em tarefas de **engenharia de software**, o **Gemini 2.5 Pro** mantém um desempenho competitivo, alcançando **53,6 % no _SWE-bench Verified_** [15], de acordo com o **leaderboard oficial SWE**. Dados divulgados pela **Google DeepMind** apontam ainda para **56,9 % em _single attempts_** e **67,2 % em _multiple attempts_**, evidenciando a eficácia do modo **_Deep Think_** em contextos de **raciocínio iterativo e resolução incremental de problemas**.

A variante **Gemini 2.5 Flash**, por sua vez, foi otimizada para **redução de latência e custo operacional**, apresentando **diminuição de 20 % a 30 % no número médio de tokens gerados** sem perdas significativas de qualidade [14]. Esta otimização torna o Flash adequado para **aplicações em tempo real** e **serviços de larga escala**, enquanto o Pro permanece direcionado para **cenários empresariais e científicos de alta complexidade**.

Com uma **janela de contexto superior a 1 milhão de tokens**, o **Gemini 2.5 Pro** posiciona-se entre os modelos com **maior capacidade de processamento contextual** atualmente disponíveis, possibilitando **análises extensas e memória conversacional persistente**. O custo médio de utilização situa-se em cerca de **1,25 USD por milhão de tokens de entrada**, valor competitivo face a modelos de gama equivalente, embora dependente da **configuração de inferência** e da **eficiência de _prompting_**.

Por fim, a **integração nativa no ecossistema Google** constitui simultaneamente uma **vantagem operacional e uma limitação tecnológica**: permite **integração otimizada com serviços como o Vertex AI e o Google Cloud Run**, mas restringe a **portabilidade e interoperabilidade** em ambientes **multicloud** ou **infraestruturas autoalojadas**, refletindo o caráter **proprietário e altamente integrado** da arquitetura Gemini.

Conclui-se, assim, que o **Gemini 2.5** se distingue por **combinar raciocínio multimodal avançado, contexto extenso e eficiência operacional**, consolidando-se como **referência no equilíbrio entre desempenho técnico e integração empresarial** no panorama dos LLMs de 2025.
# CLAUDE

O **Claude 4.5**, desenvolvido pela **Anthropic** e lançado em **setembro de 2025**, representa a evolução mais avançada da linha Claude, concebida segundo os princípios de **IA constitucional**, com ênfase na **segurança, interpretabilidade e fiabilidade operacional** [8]. A sua arquitetura combina **modelos densos de grande escala** com **mecanismos reforçados de controlo de comportamento e filtragem contextual**, consolidando a reputação da Anthropic como pioneira em **IA alinhada e verificável**.

Com uma **janela de contexto de até 200 000 tokens de entrada**, conforme indicado pela documentação oficial da Anthropic, o Claude 4.5 oferece **capacidade de raciocínio prolongado e gestão de memória conversacional extensa**, permitindo aplicações em **assistentes empresariais**, **análise documental** e **execução controlada de código**. Esta expansão de contexto representa um aumento substancial face às versões anteriores (Claude 3 Opus = 200 K, Claude 3.5 Sonnet = 200 K), mantendo a consistência do modelo em interações longas e altamente contextuais.

Em termos de desempenho, o Claude 4.5 destaca-se nos **_benchmarks_ de engenharia de software**, alcançando **70,6 % no _SWE-bench Verified_** [15] — um dos valores mais elevados entre os modelos disponíveis em 2025. De acordo com dados **auto-reportados pela Anthropic**, versões otimizadas do modelo atingem **até 77,2 %**, demonstrando melhorias consistentes em **raciocínio iterativo**, **planeamento de tarefas complexas** e **interpretação de código** [8].

O custo médio de utilização situa-se em cerca de **3 USD por milhão de tokens de entrada**, refletindo a sua orientação para **cenários empresariais de elevado desempenho e requisitos de segurança rigorosos**, em detrimento de implementações de larga escala com forte limitação orçamental. Tal posicionamento reforça o papel do Claude 4.5 como **modelo de referência para organizações que priorizam _compliance_, auditabilidade e fiabilidade operacional**.

Conclui-se, assim, que o **Claude 4.5** não visa apenas maximizar desempenho técnico, mas consolidar um **paradigma de IA segura, explicável e alinhada com princípios éticos**, posicionando-se como **solução preferencial para sistemas críticos em ambientes empresariais regulados**.

---

### **Modelos Open-Source**


# llama4

O **Llama 4 Scout**, desenvolvido pela **Meta** e lançado a **5 de abril de 2025**, representa a variante de **contexto extenso e suporte multimodal nativo** da família Llama 4 [1][2]. A sua arquitetura baseia-se em **Mixture-of-Experts (MoE)** e integra **texto, imagem e código** num único pipeline de inferência, refletindo a aposta da Meta numa abordagem de **inteligência multimodal unificada**.

Segundo o **anúncio oficial da Meta AI**, o Llama 4 Scout é capaz de processar **janelas de até 10 milhões de tokens**, um avanço substancial face às variantes anteriores e concorrentes diretos. Esta capacidade coloca-o entre os modelos mais adequados para **aplicações de memória conversacional de longo prazo** e **análise de contexto histórico extenso**, como agentes de suporte, sistemas de raciocínio contínuo ou interfaces cognitivas persistentes.

Apesar destas inovações arquiteturais, os resultados de desempenho indicam **limitações significativas em tarefas de raciocínio estruturado**. Em _benchmarks_ independentes, o **Llama 4 Scout** atinge **21,04 % no _SWE-bench Verified_** [15], demonstrando um desempenho modesto quando comparado com modelos de topo como o **GPT-5** ou o **Claude 4.5**. Em plataformas de avaliação pública, como o **ArtificialAnalysis**, apresenta cerca de **19,3 % no _AIME 2025_** [11], o que confirma a sua menor proficiência em raciocínio matemático.

A principal vantagem competitiva do Llama 4 Scout reside, contudo, no seu **baixo custo operacional**: aproximadamente **0,31 USD por tarefa** [15] ou cerca de **0,19 USD por milhão de tokens de entrada** — valores que o tornam uma opção **altamente eficiente para aplicações de grande volume ou orçamento restrito**.

Conclui-se, portanto, que o **Llama 4 Scout** não se destaca pela precisão em tarefas de raciocínio complexo, mas pela sua **eficiência económica e extraordinária capacidade de contexto**, características que o tornam **uma solução ideal para sistemas conversacionais persistentes e fluxos de trabalho orientados à memória**.

# QWEN3

O **Qwen 3**, desenvolvido pela **Alibaba Cloud** e lançado a **24 de abril de 2025**consolidando-se como um dos projetos **open-source** mais ambiciosos no ecossistema de modelos de linguagem de grande escala. Esta versão introduz uma arquitetura híbrida baseada em **Mixture-of-Experts (MoE)**, combinada com **mecanismos de raciocínio simbólico e processamento hierárquico**, refletindo o esforço da Alibaba em aproximar a inferência estatística tradicional de abordagens cognitivas mais estruturadas [6].

A família Qwen 3 cobre um **amplo espectro de variantes**, entre **0,6 milhões e 235 mil milhões de parâmetros**, abrangendo desde modelos ligeiros otimizados para _edge computing_ até modelos de propósito geral e especializados em raciocínio lógico ou engenharia de software. O modelo-base **Qwen 3-Max** destaca-se pela **extensão de contexto até 262 144 tokens**, enquanto variantes avançadas, como o **Qwen 3-Coder Plus**, alcançam **janelas de até 1 milhão de tokens**, permitindo uma gestão contextual robusta e uma utilização eficiente de memória em tarefas de programação, documentação e análise de código-fonte.

Segundo dados **auto-reportados pela Alibaba**, o Qwen 3 (235B) atinge **81,5 % no benchmark AIME 2025** [6]. Em contraste, **avaliações independentes** realizadas por plataformas como o _ArtificialAnalysis_ e o _Vals AI_ indicam valores ligeiramente superiores, próximos de **91 %**, embora estas discrepâncias devam ser interpretadas com cautela devido às diferenças de protocolo experimental. No domínio do raciocínio aplicado à engenharia de software, o **leaderboard SWE-bench Verified** [15] regista o **Qwen 3-Coder** com **55,4 %**, posicionando-o entre os modelos de **desempenho intermédio**, mas com forte consistência em tarefas de análise sintática e depuração de código.

A **principal vantagem competitiva** do Qwen 3 reside no seu **modelo de disponibilização open-source**, que elimina a dependência de APIs proprietárias e a respetiva estrutura de custos. Esta abordagem permite **execução local (_self-hosting_)**, **integração direta com sistemas empresariais** e **ajuste fino personalizado (_fine-tuning_)** sem restrições de licenciamento. Consequentemente, o **custo marginal por token é substancialmente inferior** ao dos modelos comerciais equivalentes, tornando o Qwen 3 particularmente atrativo para **investigação aplicada, experimentação e implementação híbrida em ambiente privado**.

Conclui-se, portanto, que o **Qwen 3** se destaca pela sua **abertura tecnológica, escalabilidade contextual e adaptabilidade económica**, posicionando-se como uma das **principais alternativas open-source ao ecossistema de LLMs proprietários** em 2025.

# GRANITE

O **Granite 3.2**, desenvolvido pela **IBM Research** e lançado em **fevereiro de 2025**, representa a terceira geração da linha Granite de **modelos de linguagem abertos e auditáveis**, concebida para equilibrar **desempenho técnico** com **conformidade empresarial e governança de dados** [7]. Em contraste com abordagens centradas exclusivamente em escala e poder computacional, o Granite 3.2 foi projetado com ênfase na **segurança, rastreabilidade e controlo de ciclo de vida do modelo**, tornando-se uma escolha estratégica para organizações que operam sob **fortes requisitos regulatórios e de confidencialidade**.

A arquitetura do Granite 3.2 mantém o paradigma de modelos densos, mas introduz **mecanismos internos de verificação e auditabilidade de inferência**, bem como **trilhas de conformidade integradas** com os serviços **IBM watsonx.ai** e **OpenShift AI**. Licenciado sob **Apache 2.0**, o modelo promove **transparência e liberdade de uso comercial**, permitindo **implantação _on-premises_**, **execução em ambientes privados** e **integração direta com pipelines corporativos**. Essa filosofia de design visa assegurar **reprodutibilidade e controlo soberano dos dados**, aspetos críticos em setores como finanças, saúde e administração pública.

Embora o **Granite 3.2** **não apresente métricas públicas em _benchmarks_ padronizados** — como o **AIME 2025** ou o **SWE-bench Verified** [11][15] —, a IBM destaca a sua **robustez, auditabilidade e resiliência operacional** como elementos diferenciadores. Internamente, o modelo alcança níveis elevados de **consistência de resposta**, **estabilidade de inferência** e **conformidade com políticas de governança corporativa**, sendo frequentemente escolhido como base para **implementações de IA explicável (_XAI_)** e **sistemas de apoio à decisão regulada**.

Com uma **janela de contexto de 128 000 tokens** e suporte completo a **execução local (_self-hosting_)**, o Granite 3.2 oferece **equilíbrio entre desempenho técnico e soberania organizacional**, posicionando-se como uma das **principais alternativas empresariais open-source** para a adoção segura de **modelos de linguagem em ambientes críticos**.

Conclui-se, assim, que o **Granite 3.2** não procura competir com os líderes de desempenho puro, mas afirmar-se como uma **infraestrutura de confiança, auditável e regulatoriamente sólida**, alinhada com a visão de **IA responsável e governável** promovida pela IBM.

---

### **Tabela 3.1 — Comparação de Modelos LLM (Outubro 2025)**

| **Modelo**            |                                            **AIME 2025 (%)** |                             **SWE-bench Verified (%)** | **Fonte dos Dados**                                                             |
| --------------------- | -----------------------------------------------------------: | -----------------------------------------------------: | ------------------------------------------------------------------------------- |
| **GPT-5**             |          94.3 (**ArtificialAnalysis**) / 94.6 * (**OpenAI**) |    65.0 (**ArtificialAnalysis**) / 74.9 * (**OpenAI**) | * = auto-reportado (OpenAI); ** = estimativa independente (ArtificialAnalysis)* |
| **Gemini 2.5 Pro**    | 86.7 (**ArtificialAnalysis**) / 88.0 * (**Google DeepMind**) |   53.6 (**SWE-bench**) / 67.2 * (**DeepMind interno**) | * = auto-reportado (Google); ** = benchmark independente (ArtificialAnalysis)*  |
| **Gemini 2.5 Flash**  |                                                            — |                          28.7 (**SWE-bench Verified**) | **SWE-bench** (benchmark independente)                                          |
| **Claude 4.5**        |                                                            — | 70.6 (**SWE-bench Verified**) / 77.2 * (**Anthropic**) | * = auto-reportado (Anthropic); ** = SWE-bench Verified*                        |
| **Llama 4 Maverick**  |                                19.3 (**ArtificialAnalysis**) |                          21.0 (**SWE-bench Verified**) | **ArtificialAnalysis** (AIME); **SWE-bench** (eng. software)                    |
| **Qwen 3 (Max 235B)** |         81.5 * (**Alibaba**) / 91.0 (**ArtificialAnalysis**) |                          55.4 (**SWE-bench Verified**) | * = auto-reportado (Alibaba); ** = benchmark (ArtificialAnalysis, SWE-bench)*   |
| **Granite 3.2**       |                                                            — |                                                      — | — (sem métricas públicas divulgadas)                                            |

(* self-reported; ** estimativas independentes)

---
## **Síntese**

A análise comparativa do panorama de LLMs em outubro de 2025 evidencia uma clara **bifurcação no mercado**: por um lado, **modelos proprietários** que maximizam desempenho e precisão; por outro, **modelos open-source** otimizados para custo, eficiência e adaptabilidade. Esta distinção reflete duas estratégias tecnológicas complementares:  
(1) **a verticalização e integração total** de soluções empresariais com forte suporte e fiabilidade; e  
(2) **a descentralização do poder computacional**, impulsionada por modelos abertos com capacidade de execução local.
Em suma, os LLMs contemporâneos atingiram maturidade técnica e diversidade arquitetural suficientes para sustentar casos de uso complexos de interpretação e geração de dados estruturados.

---

### **Referências**

[1] Meta AI, _The Llama 4 herd: The beginning of a new era..._, Meta Blog, abr. 2025.  
[2] Hugging Face, _Llama 4 Model Card_, 2025.  
**[3] OpenAI, “Introducing GPT-5,” Aug. 2025. [Online]. Available: https://openai.com/index/introducing-gpt-5/**
**[10] OpenAI, “API Pricing.” [Online]. Available: https://openai.com/api/pricing/**
**[13] OpenAI, “GPT-5,” Aug. 2025. [Online]. Available: https://openai.com/gpt-5/**
[4] Google DeepMind, _Gemini 2.5: Deep Think and Flash Variants_, mar.–set. 2025.  https://deepmind.google/models/gemini/pro/
[14] https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/#flash-improvements
**[15] https://www.swebench.com/?utm_source=chatgpt.com**
[5] Google AI Studio, _Gemini Pricing and Model Specs_, 2025.  
[6] Alibaba Cloud, _Qwen3 Model Card and Benchmarks_, abr.–set. 2025.  
[7] IBM Research, _Announcing Granite 3.2_, fev. 2025.  
[8] Anthropic, _Claude 4.5 Release Notes_, set. 2025.  
[9] DeepSeek AI, _DeepSeek V3 Technical Report_, set. 2025.  
**[11] AIME 2025 Benchmark, _AIME 2025 Leaderboard_, out. 2025.**  
[12] LiveCodeBench, _Leaderboard_, out. 2025.

