[[State of art]]
# **2. Abordagens Tecnológicas**

A etapa de **Geração de JSON** no _pipeline_ **NL→Dash** corresponde ao momento em que o sistema converte intenções em **configurações estruturadas e executáveis**, alinhadas com o _schema_ do motor de dashboards. A fiabilidade desta fase depende de mecanismos que assegurem **fidelidade sintática**, **validação semântica** e **baixa latência**, de modo a permitir a interação em tempo real entre o utilizador e o sistema de análise.  
As subseções seguintes apresentam o **estado da arte das abordagens tecnológicas** que sustentam essa geração estruturada, organizadas em três eixos evolutivos: _function calling nativo_, _validação externa com JSON Schema_ e _decoding estruturado_.

---

## **2.1 Function Calling Nativo**

O **Function Calling** é um mecanismo introduzido em 2023 pela OpenAI [1] e posteriormente adotado por diversos fornecedores. Em vez de gerar texto livre, o modelo **preenche argumentos de funções registadas** — por exemplo, `create_dashboard(title, layout, metrics, filters)` — devolvendo diretamente uma **estrutura JSON** conforme a assinatura da função [2][3]. Esta técnica define explicitamente os campos esperados e os respetivos tipos, permitindo que o LLM gere saídas estruturadas sem necessidade de pós-processamento complexo.

---

### **Pontos Fortes**

- **Integração direta com o motor do LLM** [11]
    
- **Conformidade estrutural:** o modelo é constrangido a gerar uma estrutura JSON válida segundo a assinatura da função, mitigando alucinações de _formato_ típicas de texto livre. Esta mitigação de erros estruturais também foi observada em abordagens de ligação LLM–API como o **Gorilla** [4], que demonstram redução de _API-call hallucinations_ ao impor assinaturas formais;
    
- **Proposta de ação explícita:** a resposta do modelo representa uma **intenção operacional clara** (a invocação de uma função), em vez de texto descritivo ambíguo que exigiria interpretação adicional; [12]

---

### **Limitações**

- **Dependência do fornecedor:** os esquemas e APIs são específicos de cada plataforma, gerando _lock-in_ e restrições de portabilidade;
    
    
- **Erros de parâmetros (falhas semânticas):** mesmo quando a sintaxe é correta, os modelos frequentemente **alucinam valores** de argumentos ou **omitem parâmetros obrigatórios**. No _benchmark_ **HammerBench** [10], centrado em cenários **multi-turn** com _tool-use chaining_, o HammerBench reporta, para **GPT-4o (Prompt)**, **PHR** entre **5,52% e 9,31%** e **PMR** entre **4,90% e 13,36%**, consoante o subtipo de dados (ver **Table 9**). As definições das métricas (_PHR_ mede nomes de parâmetros alucinados; _PMR_ mede nomes obrigatórios em falta) evidenciam **falhas persistentes na fidelidade semântica dos argumentos gerados**;
    
- **Possibilidade residual de JSON inválido:** embora o Function Calling melhore a conformidade estrutural, não garante **aderência total a um schema**.

---

### **Adequação**

O Function Calling é particularmente indicado para **prototipagem rápida** e fluxos de conversação lineares e previsíveis (_happy path_), onde o risco de ambiguidade é reduzido e os contratos de função se mantêm estáveis.  
Embora os resultados do HammerBench [10] reflitam cenários mais complexos do que o NL→Dash — com interações _multi-turn_ e encadeamento de ferramentas — eles demonstram que o Function Calling permanece vulnerável a erros de parâmetro. Assim, em sistemas que exigem **execução determinística e fiabilidade elevada**, esta técnica **não deve ser utilizada isoladamente**, devendo coexistir com camadas adicionais de validação e governação (Guardrails).

---

### **Referências**

[1] OpenAI (2023). _Function Calling and Other API Updates_. Disponível em [https://openai.com/index/function-calling-and-other-api-updates/](https://openai.com/index/function-calling-and-other-api-updates/?utm_source=chatgpt.com)  
[2] Microsoft Learn (2025). _How to Use Structured Outputs with Azure OpenAI Service_. Disponível em [https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/structured-outputs](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/structured-outputs?utm_source=chatgpt.com)  
[3] OpenAI (2025). _Structured Outputs Guide_. Disponível em [https://platform.openai.com/docs/guides/structured-outputs](https://platform.openai.com/docs/guides/structured-outputs?utm_source=chatgpt.com)  
[4] Patil, S. H. et al. (2023). _Gorilla: Large Language Model Connected with Massive APIs._ arXiv:2305.15334. Disponível em [https://arxiv.org/abs/2305.15334](https://arxiv.org/abs/2305.15334?utm_source=chatgpt.com)  
[5] JSON Schema Organization (2020). _JSON Schema Draft 2020-12 Specification._ Disponível em [https://json-schema.org/draft/2020-12](https://json-schema.org/draft/2020-12?utm_source=chatgpt.com)  
[10] Wang, Z. et al. (2024). _HammerBench: Fine-Grained Function-Calling Benchmark for Multi-Turn Tool Use._ arXiv:2412.16516. Disponível em [https://arxiv.org/abs/2412.16516](https://arxiv.org/abs/2412.16516?utm_source=chatgpt.com)
[11][Introducing Structured Outputs in the API | OpenAI] (https://openai.com/index/introducing-structured-outputs-in-the-api/)
[12] [Structured model outputs - OpenAI API](https://platform.openai.com/docs/guides/structured-outputs/examples)

---

## **2.2 Validação Externa com Guardrails**

==A **validação externa** atua como uma camada de _middleware_ entre o LLM e o sistema de destino, validando e, se necessário, reparando _outputs_ JSON após a geração. Este mecanismo constitui uma **segunda linha de defesa**, permitindo verificar tanto a conformidade sintática do JSON (por _schema_) como a plausibilidade dos valores gerados [1]–[3].==

==Ferramentas como o **Guardrails AI** implementam esta lógica por meio de _guards_, ou seja, regras declarativas (_validators_) que avaliam cada _output_ segundo critérios definidos pelo utilizador. O modelo propõe uma resposta (_draft_), sujeita a avaliação de _validators_ específicos; em caso de falha, o sistema pode executar ações corretivas pré-configuradas, incluindo rejeição automática (`exception`), regeneração condicionada ao erro (`reask`), correção automática (`fix`) ou encaminhamento para revisão manual [2], [4],== ~~[5]~~.

### **Pontos Fortes**

==**Separação de responsabilidades:** O LLM gera o _output_, mas a aceitação final depende de uma camada de validação independente; esta arquitetura implementa o princípio de _safety layering_, em que a geração e a verificação são isoladas funcionalmente para mitigar a propagação de erros sintáticos ou semânticos através da _pipeline_ [14][15].==


==**Validação multi-camada:** A validação sintática assegura o cumprimento de tipos e estruturas (por exemplo, campos e formatos obrigatórios), enquanto a validação semântica avalia a plausibilidade e relevância dos conteúdos (por exemplo, garantir que `montante` está dentro dos limites do domínio financeiro, ou que as datas são coerentes entre si) [4], [5].==

==**Reparação automática (_reask_, _fix_):** Se o _output_ for rejeitado, o Guardrails pode acionar mecanismos automáticos: `reask` instrui o LLM sobre o erro concreto, potenciando a correção autónoma (“o campo `data_vencimento` deve ser posterior a `data_emissao`”), enquanto `fix` pode truncar valores, normalizar formatos ou remover campos inválidos, reduzindo falhas manuais [2].==
### **Limitações**

==**Latência adicional:** Cada _validator_ contribui para o tempo total de resposta. Validações sintáticas, semânticas e possíveis ciclos de _reask_ podem aumentar significativamente a latência da _pipeline_. _Validators_ complexos ampliam este _overhead_ [9].==

==**Falsos positivos:** _Validators_ mal calibrados geram falsos positivos (rejeições injustificadas), degradando a eficiência e aumentando o custo. _Pipelines_ múltiplas agravam o fenómeno, tornando essencial calibrar _thresholds_ de confiança.  [15][16].==
### **Adequação**

A utilização de validação externa com Guardrails é especialmente recomendada quando:

- **A validação semântica é obrigatória:** Quando JSON válido não basta, sendo crítico garantir limites, _compliance_ e plausibilidade de cada campo [4], [5].
    
- **Rastreabilidade regulatória:** Para ambientes onde _compliance_ e _logs_ detalhados de todas as decisões automáticas são requisitos impostos por entidades reguladoras [7], [8].


---

### **Referências (IEEE – apenas fontes técnicas formais)**

[1] Guardrails AI, _Use Validators for Structured Data (JSON) Validation_, 2024. Disponível em: [https://guardrailsai.com/docs/hub/how_to_guides/structured_data_validation](https://guardrailsai.com/docs/hub/how_to_guides/structured_data_validation)  
[2] Guardrails AI, _Use on_fail Actions_, 2024. Disponível em: [https://guardrailsai.com/docs/how_to_guides/use_on_fail_actions](https://guardrailsai.com/docs/how_to_guides/use_on_fail_actions)  
[3] Guardrails AI, _validator-template (Technical Documentation)_, 2024. Disponível em: [https://github.com/guardrails-ai/validator-template](https://github.com/guardrails-ai/validator-template)  
[4] Python Instructor, _Understanding Semantic Validation with Structured Outputs_, 2025. Disponível em: [https://python.useinstructor.com/blog/2025/05/20/understanding-semantic-validation-with-structured-outputs/](https://python.useinstructor.com/blog/2025/05/20/understanding-semantic-validation-with-structured-outputs/)  
[5] SKA Observatory, _Semantic Validation_, 2024. Disponível em: [https://developer.skao.int/projects/ska-telmodel/en/1.13.0/ska-semantic/guide.html](https://developer.skao.int/projects/ska-telmodel/en/1.13.0/ska-semantic/guide.html)  
[6] Aveni, _AI Guardrails and Monitoring That Actually Work in Financial Services_, 2025. Disponível em: [https://aveni.ai/blog/ai-guardrails-and-monitoring-that-actually-work-in-financial-services/](https://aveni.ai/blog/ai-guardrails-and-monitoring-that-actually-work-in-financial-services/)  
[7] arXiv, _Assessing the Auditability of AI-Integrating Systems_, 2025. Disponível em: [https://arxiv.org/html/2411.08906v1](https://arxiv.org/html/2411.08906v1)  
[8] Cloudflare, _Guardrails in AI Gateway_, 2025. Disponível em: [https://blog.cloudflare.com/guardrails-in-ai-gateway/](https://blog.cloudflare.com/guardrails-in-ai-gateway/)  
[9] Guardrails AI, _Performance Considerations_, 2024. Disponível em: [https://guardrailsai.com/docs/concepts/performance](https://guardrailsai.com/docs/concepts/performance)  
[10] Dynamo, _How to Evaluate Guardrails_, 2024. Disponível em: [https://www.dynamo.ai/models/07-how-to-evaluate-guardrails](https://www.dynamo.ai/models/07-how-to-evaluate-guardrails)  
[11] YLD, _Semantics and Syntax in GenAI Applications_, 2025. Disponível em: [https://www.yld.io/blog/semantics-and-syntax-in-genai-applications](https://www.yld.io/blog/semantics-and-syntax-in-genai-applications)  
[12] Bud Ecosystem, _Guardrail Testing, Validating, Tools and Frameworks_, 2025. Disponível em: [https://blog.budecosystem.com/llm-guardrails-guardrail-testing-validating-tools-and-frameworks/](https://blog.budecosystem.com/llm-guardrails-guardrail-testing-validating-tools-and-frameworks/)  
[13] Cloudflare, _Guardrails in AI Gateway (Case Example)_, 2025. Disponível em: [https://blog.cloudflare.com/guardrails-in-ai-gateway/](https://blog.cloudflare.com/guardrails-in-ai-gateway/)
[14] Bai, Y. et al., _Constitutional AI: Harmlessness from AI Feedback_, arXiv preprint arXiv:2212.08073, 2022. Disponível em: https://arxiv.org/abs/2212.08073
[15 ]Xu, A. et al., _Designing Multi-layered Runtime Guardrails for Foundation Model Applications_, arXiv preprint arXiv:2408.02205, 2024.
[16][*usenixsecurity25-villa.pdf](https://www.usenix.org/system/files/usenixsecurity25-villa.pdf)
[17][*pdf](https://openreview.net/pdf?id=zJAm9nLdaQ)

---

## **2.3 Decoding Estruturado / Guiado por Gramática**

**intro**

Decoding estruturado guiado por gramática, conhecido como grammar-constrained decoding (GCD), é uma técnica que modifica o mecanismo interno de geração dos modelos de linguagem (LLMs) para garantir que o output segue regras sintáticas pré-definidas[2305.13971]. Ao contrário do texto livre, cada token é gerado de modo a não quebrar a estrutura formal (por exemplo, de JSON, SQL, linguagens específicas de domínio), bloqueando sequências inválidas à medida que são construídas. Nos últimos anos, GCD tem sido promovido como alternativa à validação pós-processamento ou fine-tuning quando o formato do resultado é crítico.[nexastack+4](https://www.nexastack.ai/blog/structured-decoding-with-vllm)​

---

## Pontos Fortes

- ==**Garantia estrutural**. GCD oferece uma garantia formal de que o output está sempre conforme uma gramática definida (JSON válido, código compilável, árvore sintática correta)[2305.13971].== Isto elimina a necessidade de pós-processar ou corrigir erros que podem tornar outputs inutilizáveis.[aidancooper+3](https://www.aidancooper.co.uk/constrained-decoding/)​
    
- **Adaptação rápida sem re-treino**. Uma das principais vantagens é poder adaptar modelos pré-treinados para novas estruturas apenas mudando a gramática, sem treinar o modelo, reduzindo custos, tempo e dependência de grandes datasets. Na literatura recente, frameworks como vLLM e Outlines mostram que esta abordagem funciona para múltiplos formatos e aplicações.[leewayhertz+3](https://www.leewayhertz.com/structured-outputs-in-llms/)​
    
- ==**Generalidade e flexibilidade**. O artigo [2305.13971] argumenta que gramáticas formais podem descrever o espaço de output para uma vasta gama de tarefas[2305.13971].==
    
- **Desempenho competitivo em low-resource e few-shot**. ==Estudos sugerem que GCD pode alcançar performance próxima do fine-tuning especializado em alguns cenários, especialmente onde há poucos dados ou orçamentos restritos[2305.13971]==. ==Em tarefas de extração clínica com pouco treino, GCD pode dar saltos de F1 de 0.062 para 0.413 ou de 0.102 para 0.47 sem grandes esforços de labeling.[pmc.ncbi.nlm.nih](https://pmc.ncbi.nlm.nih.gov/articles/PMC11747381/)==​

---

## Limitações

- ==**Sintaxe ≠ Semântica**. GCD garante estrutura, não significado; podes ter outputs formalmente corretos e semanticamente irrelevantes, inconsistentes ou até perigosos[2305.13971].==​
    
- ==**Cobertura e generalização escassa**. Os estudos frequentemente demonstram sucesso apenas em algumas tarefas (parsing, geração de código, templates médicos); extrapolar para "todas as tarefas estruturadas" continua sem suporte forte na literatura universal[2305.13971]==.[pmc.ncbi.nlm.nih](https://pmc.ncbi.nlm.nih.gov/articles/PMC11747381/)​
    
- ==**Incompatível com APIs comerciais**. O artigo e vários relatos práticos frisam que GCD geralmente não pode ser usado com modelos de cloud/API como GPT-4 ou Claude, porque não é possível controlar as escolhas internas de decoding[2305.13971].==[agenta+1](https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms)​
    
- ==**Overhead e latência dependentes da gramática**. A introdução de parsing e masking pode aumentar tempo de resposta substancialmente em gramáticas complexas ou tarefas de alta escalabilidade==, limitando uso em produção real-time[2305.13971].[aclanthology+3](https://aclanthology.org/2025.acl-long.551.pdf)​
    

---

## Adequação

**Quando usar GCD**:

- ==Tarefas em que garantir o formato do output é mais importante do que tratar significado (ficheiros estruturados, templates fixos, parsing, geração de código)[2305.13971]==.[nexastack+1](https://www.nexastack.ai/blog/structured-decoding-with-vllm)​
    
- Prototipagem rápida ou ambientes low-resource, onde fine-tuning é caro, difícil ou impossível[2305.13971].[agenta+1](https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms)​
    
- Workflows locais com pleno controlo sobre o modelo, integração flexível entre diferentes estruturas e necessidades.[aidancooper+2](https://www.aidancooper.co.uk/constrained-decoding/)​
    

**Quando evitar ou complementar GCD**:

- Contextos dependentes de APIs comerciais, sem acesso ao processo de decoding interno[2305.13971].[agenta+1](https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms)​
    
- Tarefas que exigem compreensão semântica ou dependências contextuais difíceis de formalizar gramaticalmente[2305.13971].[aclanthology+3](https://aclanthology.org/2025.acl-srw.59.pdf)​
    
- Grandes sistemas distribuídos ou aplicações real-time, onde overhead pode ser crítico.[openreview+3](https://openreview.net/forum?id=L6CYAzpO1k)​
    
- Casos onde modelos fine-tuned oferecem desempenho e robustez superiores, especialmente em outputs com múltiplos níveis de dependência lógica.[proceedings.neurips+2](https://proceedings.neurips.cc/paper_files/paper/2024/file/2bdc2267c3d7d01523e2e17ac0a754f3-Paper-Conference.pdf)​
    

---

**Considerações Críticas**

A literatura realça o potencial de GCD para expandir a aplicabilidade dos LLMs em tarefas estruturadas e reduzir barreiras de entrada associadas ao fine-tuning, mas alerta que a garantia estrutural não é sinónimo de adequação universal. Devem ser ponderados os trade-offs e limitações reais, integrando GCD como solução parcial, nunca como substituto total de outras técnicas — e sempre fundamentando empiricamente a adoção para o teu cenário específico[2305.13971].[aidancooper+3](https://www.aidancooper.co.uk/constrained-decoding/)​

1. [https://www.nexastack.ai/blog/structured-decoding-with-vllm](https://www.nexastack.ai/blog/structured-decoding-with-vllm)
2. [https://www.aidancooper.co.uk/constrained-decoding/](https://www.aidancooper.co.uk/constrained-decoding/)
3. [https://www.leewayhertz.com/structured-outputs-in-llms/](https://www.leewayhertz.com/structured-outputs-in-llms/)
4. [https://aclanthology.org/2023.emnlp-main.674.pdf](https://aclanthology.org/2023.emnlp-main.674.pdf)
5. [https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms](https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms)
6. [https://pmc.ncbi.nlm.nih.gov/articles/PMC11747381/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11747381/)
7. [https://acl-bg.org/proceedings/2025/RANLP%202025/pdf/2025.ranlp-1.124.pdf](https://acl-bg.org/proceedings/2025/RANLP%202025/pdf/2025.ranlp-1.124.pdf)
8. [https://research.tue.nl/en/publications/grammar-constrained-decoding-makes-large-language-models-better-l](https://research.tue.nl/en/publications/grammar-constrained-decoding-makes-large-language-models-better-l)
9. [https://aclanthology.org/2025.acl-srw.59.pdf](https://aclanthology.org/2025.acl-srw.59.pdf)
10. [https://aclanthology.org/2025.acl-long.551.pdf](https://aclanthology.org/2025.acl-long.551.pdf)
11. [https://openreview.net/forum?id=L6CYAzpO1k](https://openreview.net/forum?id=L6CYAzpO1k)
12. [https://arxiv.org/pdf/2502.05111.pdf](https://arxiv.org/pdf/2502.05111.pdf)
13. [https://proceedings.neurips.cc/paper_files/paper/2024/file/2bdc2267c3d7d01523e2e17ac0a754f3-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2024/file/2bdc2267c3d7d01523e2e17ac0a754f3-Paper-Conference.pdf)

---

## **3. Comparação das Abordagens**



---

## **4. Discussão e Tendência Tecnológica**


---

## **5. Conclusão**



---

## **Referências**

[1] OpenAI — _Function Calling Guide_ (2025). Disponível em: [https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)  
[2] OpenAI — _Function Calling in the API (Help Center)_, 2025.  
[3] Guardrails AI — _Your Enterprise AI Needs Guardrails_ (2025).  
[4] JSON Schema — _Official Specification_ (2024).  
[5] Microsoft Research — _Guidance: Controlling LM Output_, 2025.  
[6] Dottxt AI — _Outlines: Structured Outputs Complying with JSON Schema_, 2025.  
[7] BentoML — _Structured Decoding in vLLM: A Gentle Introduction_, 2025.  
[8] Raspanti, L. et al., _Grammar-Constrained Decoding Makes LLMs Better Logical Parsers_, _ACL Industry Track 2025_.  
[9] Li et al. (2025). JSONSchemaBench: A Rigorous Benchmark... arXiv:2501.10868.
[10] Wang et al. (2024). HammerBench: Fine-Grained Function-Calling... arXiv:2412.16516. [Cita em 2.1 limitações: "25% ↓ falhas mobile/real-time"]


