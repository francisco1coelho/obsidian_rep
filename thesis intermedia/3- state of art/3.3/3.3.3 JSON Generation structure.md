[[State of art]]
# **2. Technological Approaches**

The **JSON Generation** stage in the **NL→Dash** pipeline represents the moment when the system converts user intents into **structured and executable configurations**, aligned with the dashboard engine’s schema. The reliability of this stage depends on mechanisms that ensure **syntactic fidelity**, **semantic validation**, and **low latency**, allowing real-time interaction between the user and the analytics system.  
The following subsections present the **state of the art in technological approaches** that support this structured generation, organized along three evolutionary axes: _native function calling_, _external validation with JSON Schema_, and _structured decoding_.

---

## **2.1 Function Calling**

**Function Calling** is a mechanism introduced in 2023 by OpenAI [1] and quickly adopted by other providers such as Anthropic (Claude), Google (Gemini), and Microsoft (Azure OpenAI). In this paradigm, the developer pre-registers available functions through **JSON Schemas** that specify the function’s name, description, parameters (data types, required fields, constraints), and optionally the return type. Instead of generating free text, the model selects the appropriate function and fills in the arguments according to the schema, returning a validated JSON structure. [2][3]

---

### **Strengths**

- **Direct integration with the LLM engine** [6]
    
- **Structural compliance:** the model is constrained to produce a valid JSON structure that follows the function signature, mitigating _format hallucinations_ typical of free-text generation. This mitigation of structural errors has also been observed in LLM–API linking approaches such as **Gorilla** [4], which demonstrated reduced _API-call hallucinations_ by enforcing syntactic validation.
    
- **Explicit action proposal:** the model’s response represents a **clear operational intent** (a function invocation) instead of ambiguous descriptive text requiring additional interpretation. [7]
    

---

### **Limitations**

- **Parameter errors (semantic failures):** even when syntax is correct, models often **hallucinate argument values** or **omit required parameters**. In the **HammerBench** benchmark [5], focused on **multi-turn** _tool-use chaining_ scenarios, HammerBench reports, for **GPT-4o (Prompt)**, **PHR** between **5.52% and 9.31%** and **PMR** between **4.90% and 13.36%**, depending on the data subtype (see **Table 9** in [5]). The metric definitions (_PHR_ measures hallucinated parameter names; _PMR_ measures missing required parameters) highlight **persistent issues with semantic fidelity in generated arguments**.
    
- **Residual risk of invalid JSON:** although Function Calling improves structural conformance, it does not guarantee **full adherence to a schema**.
    

---

### **Suitability**

Function Calling is particularly suitable for **rapid prototyping** and **linear, predictable conversational flows** (_happy paths_), where ambiguity is reduced and function contracts remain stable.  
However, the results from HammerBench [5] show that Function Calling remains vulnerable to parameter-related errors. Therefore, in systems that require **deterministic execution and high reliability**, this technique **should not be used in isolation**, but rather combined with additional validation layers.

---

### **References**

[1] OpenAI (2023). _Function Calling and Other API Updates_. Available at [https://openai.com/index/function-calling-and-other-api-updates/](https://openai.com/index/function-calling-and-other-api-updates/)  
[2] OpenAI (2025). Function calling. Available at [https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)  
[3] OpenAI (2025). _Structured Outputs Guide_. Available at [https://platform.openai.com/docs/guides/structured-outputs](https://platform.openai.com/docs/guides/structured-outputs)  
[4] Patil, S. H. et al. (2023). _Gorilla: Large Language Model Connected with Massive APIs._ arXiv:2305.15334. Available at [https://arxiv.org/abs/2305.15334](https://arxiv.org/abs/2305.15334)  
[5] Wang, Z. et al. (2024). _HammerBench: Fine-Grained Function-Calling Benchmark for Multi-Turn Tool Use._ arXiv:2412.16516. Available at [https://arxiv.org/abs/2412.16516](https://arxiv.org/abs/2412.16516?utm_source=chatgpt.com)  
[6] [Introducing Structured Outputs in the API | OpenAI](https://openai.com/index/introducing-structured-outputs-in-the-api/)  
[7] [Structured model outputs - OpenAI API](https://platform.openai.com/docs/guides/structured-outputs/examples)

---

## **2.2 Guardrails**

**Guardrails** are **post-processing validation systems** that operate on the inputs and outputs of LLMs, verifying structural compliance (e.g., adherence to JSON Schema) and, in some cases, applying limited semantic validation [6]. When non-conforming outputs are detected, repair mechanisms may include: (i) **automatic syntactic correction** via libraries such as JSON Repair [17], (ii) **regeneration** by re-asking the LLM [8], or (iii) **rule-based editing** [6]. **Importantly**, these mechanisms present documented limitations, including partial effectiveness (around 61% in complex tasks) and a significant increase in latency [9][10].

Tools such as **Guardrails AI** implement this logic through _guards_—declarative rules (_validators_) that assess each output according to user-defined criteria. The model produces a _draft_ response, which is then evaluated by specific _validators_; if validation fails, the system can execute preconfigured corrective actions, including automatic rejection (`exception`), conditional regeneration based on the detected error (`reask`), automatic correction (`fix`), or routing to manual review [1][2].

---

### **Strengths**

**Separation of responsibilities:** The LLM generates the output, but final acceptance depends on an independent validation layer. This architecture embodies the _safety layering_ principle, in which generation and verification are functionally isolated to mitigate the propagation of syntactic or semantic errors throughout the pipeline [11][5].

**Multi-layer validation:** Syntactic validation ensures compliance with required types and structures (e.g., mandatory fields and formats), while semantic validation assesses the plausibility and consistency of the content (e.g., ensuring that `amount` values are within valid financial limits or that dates are temporally consistent) [2][3].

**Automatic repair (_reask_, _fix_):** If an output is rejected, Guardrails can trigger automated mechanisms. `reask` instructs the LLM on the specific error, enabling self-correction (“the field `due_date` must be later than `issue_date`”), while `fix` can truncate values, normalize formats, or remove invalid fields—reducing the need for manual intervention [1].

---

### **Limitations**

**Additional latency:** Each _validator_ contributes to total response time. Syntactic and semantic validations, combined with possible _reask_ cycles, can significantly increase pipeline latency. Complex validators amplify this overhead [4].

**False positives:** Poorly tuned _validators_ can generate false positives (unjustified rejections), reducing efficiency and increasing operational cost [5].

---

### **Suitability**

External validation through Guardrails is particularly recommended when:

- **Semantic validation is mandatory:** When producing syntactically valid JSON is insufficient, and domain constraints must be enforced.
    
- **Regulatory traceability:** In environments where _compliance_ and detailed logs of all automated decisions are required by regulatory bodies.
    

---

### **References (IEEE – formal technical sources only)**

[1] Guardrails AI, _Use on_fail Actions_, 2024. Available: [https://guardrailsai.com/docs/how_to_guides/use_on_fail_actions](https://guardrailsai.com/docs/how_to_guides/use_on_fail_actions)  
[2] Python Instructor, _Understanding Semantic Validation with Structured Outputs_, 2025. Available: [https://python.useinstructor.com/blog/2025/05/20/understanding-semantic-validation-with-structured-outputs/](https://python.useinstructor.com/blog/2025/05/20/understanding-semantic-validation-with-structured-outputs/)  
[3] SKA Observatory, _Semantic Validation_, 2024. Available: [https://developer.skao.int/projects/ska-telmodel/en/1.13.0/ska-semantic/guide.html](https://developer.skao.int/projects/ska-telmodel/en/1.13.0/ska-semantic/guide.html)  
[4] Guardrails AI, _Performance Considerations_, 2024. Available: [https://guardrailsai.com/docs/concepts/performance](https://guardrailsai.com/docs/concepts/performance)  
[5] Xu, A. et al., _Designing Multi-layered Runtime Guardrails for Foundation Model Applications_, _arXiv preprint_ arXiv:2408.02205, 2024. Available: [https://arxiv.org/pdf/2408.02205v2](https://arxiv.org/pdf/2408.02205v2)  
[6] _Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences_, _arXiv preprint_ arXiv:2502.08142, 2025. Available: [https://arxiv.org/html/2502.08142](https://arxiv.org/html/2502.08142)  
[7] _@toolsycc/json-repair - npm_, 2024. Available: [https://www.npmjs.com/package/@toolsycc/json-repair](https://www.npmjs.com/package/@toolsycc/json-repair)  
[8] _The Guard | Your Enterprise AI needs Guardrails_, 2024. Available: [https://www.guardrailsai.com/docs/concepts/guard](https://www.guardrailsai.com/docs/concepts/guard)  
[9] _SoK: Evaluating Jailbreak Guardrails for Large Language Models_, _arXiv preprint_ arXiv:2506.10597, 2025. Available: [https://arxiv.org/pdf/2506.10597v2](https://arxiv.org/pdf/2506.10597v2)  
[10] _arXiv preprint_ arXiv:2502.18878, 2025. Available: [https://arxiv.org/pdf/2502.18878](https://arxiv.org/pdf/2502.18878)  
[11] Bai, Y. et al., _Constitutional AI: Harmlessness from AI Feedback_, _arXiv preprint_ arXiv:2212.08073, 2022. Available: [https://arxiv.org/abs/2212.08073](https://arxiv.org/abs/2212.08073)


[1] Guardrails AI, _Use Validators for Structured Data (JSON) Validation_, 2024. Disponível em: [https://guardrailsai.com/docs/hub/how_to_guides/structured_data_validation](https://guardrailsai.com/docs/hub/how_to_guides/structured_data_validation)  
==[2] Guardrails AI, _Use on_fail Actions_, 2024. Disponível em: [https://guardrailsai.com/docs/how_to_guides/use_on_fail_actions](https://guardrailsai.com/docs/how_to_guides/use_on_fail_actions)==  
[3] Guardrails AI, _validator-template (Technical Documentation)_, 2024. Disponível em: [https://github.com/guardrails-ai/validator-template](https://github.com/guardrails-ai/validator-template)  
==[4] Python Instructor, _Understanding Semantic Validation with Structured Outputs_, 2025. Disponível em: [https://python.useinstructor.com/blog/2025/05/20/understanding-semantic-validation-with-structured-outputs/](https://python.useinstructor.com/blog/2025/05/20/understanding-semantic-validation-with-structured-outputs/)==  
==[5] SKA Observatory, _Semantic Validation_, 2024. Disponível em: [https://developer.skao.int/projects/ska-telmodel/en/1.13.0/ska-semantic/guide.html](https://developer.skao.int/projects/ska-telmodel/en/1.13.0/ska-semantic/guide.html)==  
==[9] Guardrails AI, _Performance Considerations_, 2024. Disponível em: [https://guardrailsai.com/docs/concepts/performance](https://guardrailsai.com/docs/concepts/performance)==  
[13] Cloudflare, _Guardrails in AI Gateway (Case Example)_, 2025. Disponível em: [https://blog.cloudflare.com/guardrails-in-ai-gateway/](https://blog.cloudflare.com/guardrails-in-ai-gateway/)
[14] Bai, Y. et al., _Constitutional AI: Harmlessness from AI Feedback_, arXiv preprint arXiv:2212.08073, 2022. Disponível em: https://arxiv.org/abs/2212.08073
==[15 ]Xu, A. et al., _Designing Multi-layered Runtime Guardrails for Foundation Model Applications_, arXiv preprint arXiv:2408.02205, 2024. [2408.02205v2](https://arxiv.org/pdf/2408.02205v2)==
[16][*usenixsecurity25-villa.pdf](https://www.usenix.org/system/files/usenixsecurity25-villa.pdf)
[17][*pdf](https://openreview.net/pdf?id=zJAm9nLdaQ)
==[18] [2502.08142v1.pdf]([Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences](https://arxiv.org/html/2502.08142))==
==[19] [@toolsycc/json-repair - npm](https://www.npmjs.com/package/@toolsycc/json-repair)==
==[20] [The Guard | Your Enterprise AI needs Guardrails](https://www.guardrailsai.com/docs/concepts/guard)==
==[21] [SoK: Evaluating Jailbreak Guardrails for Large Language Models](https://arxiv.org/pdf/2506.10597v2)==
==[22] [2502.18878](https://arxiv.org/pdf/2502.18878)==

---

## **2.3 Structured / Grammar-Guided Decoding**

**Structured decoding guided by grammar**, also known as **grammar-constrained decoding (GCD)**, is a technique that modifies the internal generation mechanism of large language models (LLMs) to ensure that the output follows predefined syntactic rules [4].

---

## **Strengths**

- **Structural guarantee.** GCD provides a formal assurance that the output will always conform to a defined grammar (e.g., valid JSON, compilable code, correct syntax tree) [4].
    
- **Generality and flexibility.** The study [4] argues that formal grammars can describe the output space for a wide range of tasks.
    
- **Competitive performance in low-resource and few-shot settings.** Research suggests that GCD can achieve performance close to specialized fine-tuning in certain cases, especially where data or computational budgets are limited [4]. In clinical extraction tasks with limited training data, GCD has been shown to raise F1 scores from 0.062 to 0.413 or from 0.102 to 0.47 [1].
    

---

## **Limitations**

- **Syntax ≠ Semantics.** GCD guarantees structure, not meaning; the outputs may be formally correct but semantically irrelevant or inconsistent [4][2].
    
- **Incompatibility with commercial APIs.** The literature and practical reports emphasize that GCD generally cannot be applied to cloud-based or API-based models such as GPT-4 or Claude, since decoding choices cannot be controlled directly [4].
    
- **Grammar-dependent overhead and latency.** The introduction of parsing and masking can substantially increase response time in complex grammars or large-scale applications [4].
    

---

## **Suitability**

**When to use GCD:**

- Tasks where ensuring the output format is more important than the meaning (structured files, fixed templates, parsing, code generation) [4].
    
- Rapid prototyping or low-resource environments where fine-tuning is expensive, impractical, or impossible [4].
    

**When to avoid or complement GCD:**

- Scenarios relying on commercial APIs [4].
    
- Tasks requiring semantic understanding or context-dependent reasoning that cannot be easily formalized via grammar [4][2].
    
- Large-scale distributed systems or real-time applications where overhead is critical [3].
    

---

## **Critical Considerations**

The literature highlights the potential of GCD to broaden the applicability of LLMs for structured tasks and reduce the entry barriers associated with fine-tuning. However, it cautions that structural guarantees do not equate to universal adequacy. Trade-offs and real-world limitations should be carefully considered, integrating GCD as a **partial solution**, not a total substitute for other techniques — and its adoption should always be empirically justified for the specific context [4].

---

### **References**

[1] _PMC_, “LLM-based Clinical Information Extraction Study,” 2025. Available: [https://pmc.ncbi.nlm.nih.gov/articles/PMC11747381/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11747381/)  
[2] _ACL Anthology_, “Evaluating Grammar-Constrained Decoding in Semantic Tasks,” 2025. Available: [https://aclanthology.org/2025.acl-srw.59.pdf](https://aclanthology.org/2025.acl-srw.59.pdf)  
[3] _OpenReview_, “Structured Decoding in Large-Scale Systems,” 2025. Available: [https://openreview.net/forum?id=L6CYAzpO1k](https://openreview.net/forum?id=L6CYAzpO1k)  
[4] _Zhou et al._, “Grammar-Constrained Decoding for Large Language Models,” _arXiv preprint_ arXiv:2305.13971, 2023. Available: [https://arxiv.org/pdf/2305.13971](https://arxiv.org/pdf/2305.13971)


1. [https://www.nexastack.ai/blog/structured-decoding-with-vllm](https://www.nexastack.ai/blog/structured-decoding-with-vllm)
2. [https://www.aidancooper.co.uk/constrained-decoding/](https://www.aidancooper.co.uk/constrained-decoding/)
3. [https://www.leewayhertz.com/structured-outputs-in-llms/](https://www.leewayhertz.com/structured-outputs-in-llms/)
4. [https://aclanthology.org/2023.emnlp-main.674.pdf](https://aclanthology.org/2023.emnlp-main.674.pdf)
5. [https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms](https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms)
6. ==[https://pmc.ncbi.nlm.nih.gov/articles/PMC11747381/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11747381/)==
7. [https://acl-bg.org/proceedings/2025/RANLP%202025/pdf/2025.ranlp-1.124.pdf](https://acl-bg.org/proceedings/2025/RANLP%202025/pdf/2025.ranlp-1.124.pdf)
8. [https://research.tue.nl/en/publications/grammar-constrained-decoding-makes-large-language-models-better-l](https://research.tue.nl/en/publications/grammar-constrained-decoding-makes-large-language-models-better-l)
9. ==[https://aclanthology.org/2025.acl-srw.59.pdf](https://aclanthology.org/2025.acl-srw.59.pdf)==
10. [https://aclanthology.org/2025.acl-long.551.pdf](https://aclanthology.org/2025.acl-long.551.pdf)
11. ==[https://openreview.net/forum?id=L6CYAzpO1k](https://openreview.net/forum?id=L6CYAzpO1k)==
12. [https://arxiv.org/pdf/2502.05111.pdf](https://arxiv.org/pdf/2502.05111.pdf)
13. [https://proceedings.neurips.cc/paper_files/paper/2024/file/2bdc2267c3d7d01523e2e17ac0a754f3-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2024/file/2bdc2267c3d7d01523e2e17ac0a754f3-Paper-Conference.pdf)
14. ==[2305.13971](https://arxiv.org/pdf/2305.13971)==

---

## **3. Comparação das Abordagens**



---

## **4. Discussão e Tendência Tecnológica**


---

## **5. Conclusão**



---

## **Referências**

[1] OpenAI — _Function Calling Guide_ (2025). Disponível em: [https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)  
[2] OpenAI — _Function Calling in the API (Help Center)_, 2025.  
[3] Guardrails AI — _Your Enterprise AI Needs Guardrails_ (2025).  
[4] JSON Schema — _Official Specification_ (2024).  
[5] Microsoft Research — _Guidance: Controlling LM Output_, 2025.  
[6] Dottxt AI — _Outlines: Structured Outputs Complying with JSON Schema_, 2025.  
[7] BentoML — _Structured Decoding in vLLM: A Gentle Introduction_, 2025.  
[8] Raspanti, L. et al., _Grammar-Constrained Decoding Makes LLMs Better Logical Parsers_, _ACL Industry Track 2025_.  
[9] Li et al. (2025). JSONSchemaBench: A Rigorous Benchmark... arXiv:2501.10868.
[10] Wang et al. (2024). HammerBench: Fine-Grained Function-Calling... arXiv:2412.16516. [Cita em 2.1 limitações: "25% ↓ falhas mobile/real-time"]


