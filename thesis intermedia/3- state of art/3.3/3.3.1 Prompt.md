[[State of art]]

# **Foundations and Related Technologies**

## **Introduction**

The first stage of the **NL→Dash** pipeline is responsible for interpreting the user’s initial request expressed in natural language. Its goal is to identify the **main intent** of the utterance (“create dashboard”, “update metric”, “apply filter”, “adjust threshold”) and to **extract minimal contextual information** (for example, the domain of the request or the type of metric mentioned), preparing the system for later stages of semantic mapping and structured generation.

## **Technological Analysis (LLMs with Public API)**

The execution of this phase depends on the ability of **Large Language Models (LLMs)** to understand intents, handle ambiguity, and maintain dialogue coherence. Selecting models with a **public API** is essential to enable rapid integration, controlled cost, and ease of experimentation.

The following section presents a critical analysis of the most relevant models, considering linguistic performance, inference latency, cost, context window size, and suitability to the project’s objectives.

### **Mistral 7B (Instruct)**

**Platform/API:** available via the **Hugging Face Inference API** and various _inference hosting_ services; supports _self-hosting_.  
**Advantages:** lightweight and fast model, ideal for **real-time** interactions; responds well to task-oriented _prompt engineering_; low cost and excellent performance in intent classification.  
**Limitations:** short context window, which limits the ability to retain conversational history; less semantic depth in ambiguous prompts.  

### **Llama 3 (8B/70B)**

**Platform/API:** available on **Hugging Face**, **Together AI**, **Fireworks AI**, and other providers; supports _self-hosting_.  
**Advantages:** excellent _instruction-following_, strong contextual coherence, and high-quality text generation; extended context window and competitive latency in the 8B version.  
**Limitations:** the 70B variant is heavy for real-time operation; may require additional _decoding_ constraints to ensure consistent output formatting.  

### **Falcon 7B/40B**

**Platform/API:** open-weight models supported by multiple providers and **Hugging Face Inference**.  
**Advantages:** stable and well-documented architecture; the 40B model provides solid linguistic coverage.  
**Limitations:** higher **inference latency** and less refined instruction-tuning compared to newer models; weaker dialogue quality without dedicated _fine-tuning_.  

### **Gemma 2 (Google)**

**Platform/API:** available through **Google Cloud Inference API** and **Hugging Face**.  
**Advantages:** strong at interpreting user requests, high _instruction-following_ capability, and good contextual reasoning; suitable for _few-shot prompting_; enhanced safety features.  
**Limitations:** higher GPU usage and increased latency under heavy concurrency.  

### **Mixtral (MoE 8×7B)**

**Platform/API:** available via **Hugging Face** and managed inference providers; requires specific infrastructure for _self-hosting_.  
**Advantages:** the _Mixture-of-Experts_ architecture combines **large-model quality** with **competitive latency**; highly effective for dialogue management and context coherence; good control over output format.  
**Limitations:** higher setup complexity and additional tuning required for inference stability.  

---

## **Comparison and Fit within NL→Dash**

Performance in this stage must balance **inference speed** with **intent detection accuracy**.

- **Mistral 7B** and **Llama 3 8B** offer the best performance-to-cost ratio, making them ideal for **real-time systems** handling short, well-defined requests.
    
- **Gemma 2** and **Mixtral** excel in scenarios involving **continuous dialogue**, larger **context windows**, and **ambiguous language interpretation**.
    
- **Falcon 7B/40B**, while functional, shows lower consistency in natural-language intent generation without dedicated _fine-tuning_.
    

In the context of the NL→Dash pipeline—where the user may issue successive commands (“remove this metric”, “add a bar chart”) and the system must maintain context—**dialogue coherence and low latency** are key requirements. Therefore, **Llama 3 8B** or **Mistral 7B** represent ideal baseline options for initial deployment, with potential evolution toward **Mixtral** or **Gemma 2** as contextual dialogue complexity or semantic load increases.

---

## **Critical Synthesis**

This stage forms the interactive core of the **NL→Dash** system, where dialogue naturalness and intent accuracy determine the overall quality of the pipeline. Among the models evaluated, **Llama 3 (8B)** offers the best **technical trade-off** between cost, latency, and interpretive consistency, making it ideal for **conversational interfaces** and **real-time intent interpretation**.

**Mistral 7B** stands out as a lightweight, cost-effective alternative for low-latency operation, while **Gemma 2** and **Mixtral** are better suited for contexts requiring deeper contextual reasoning and extended dialogue handling.

---



# **3.1 Fundamentos e Tecnologias Relacionadas**

A primeira etapa do pipeline **NL→Dash** é responsável por interpretar o pedido inicial do utilizador em linguagem natural, identificando a **intenção principal** (“criar dashboard”, “atualizar métrica”, “aplicar filtro”, “ajustar limiar”) e extraindo **informação contextual mínima** (como o domínio, o tipo de métrica ou a janela temporal relevante). O desempenho desta fase depende diretamente da capacidade dos **Modelos de Linguagem de Grande Escala (LLMs)** para compreender intenções, resolver ambiguidades e manter coerência conversacional ao longo de múltiplas interações.

A utilização de **LLMs com API pública** é essencial em projetos que exigem integração rápida, controlo de custos e experimentação contínua. No entanto, a seleção do modelo deve equilibrar **fidelidade estrutural** (geração de saídas JSON válidas), **latência de inferência** e **robustez semântica**. Importa salientar que muitos resultados reportados por fornecedores são obtidos em ambientes controlados, podendo diferir do desempenho real em tarefas específicas como a do **NL→Dash**, centrada na geração e refinamento de configurações de dashboards em tempo real. Ainda existe uma lacuna notória de benchmarks independentes que avaliem **consistência multi-turn** e **validação estrutural** em contextos de interação contínua, como os contact centers descritos no projeto.

---

## **Panorama Atual dos LLMs com API Pública**

Em outubro de 2025, o ecossistema de LLMs caracteriza-se por diversidade arquitetural e rápida iteração de versões, com destaque para **arquiteturas Mixture-of-Experts (MoE)**, **modos de raciocínio seletivo** e **capacidades multimodais**. As famílias **Llama 4**, **Qwen3**, **DeepSeek V3** e **Granite 3.2** (open-source) e os modelos **GPT-5**, **Gemini 2.5** e **Claude 4.5** (proprietários) representam os eixos principais de comparação no mercado atual. Para contextualização, inclui-se ainda o **Grok-4 Heavy** (xAI), modelo de grande escala lançado em junho de 2025, focado em raciocínio matemático mas sem API pública generalizada.

As métricas comparativas apresentadas nesta secção foram obtidas a partir de **model cards oficiais**, **leaderboards independentes** (SWE-bench Verified, AIME 2025, LiveCodeBench) e **documentação de API pública**, consultadas em outubro de 2025. Valores assinalados com (*) são _self-reported_ pelos fornecedores e (~) estimativas independentes.


---

### **Modelos Proprietários**

O **GPT-5**, lançado pela OpenAI a 7 de agosto de 2025, representa o padrão de referência em raciocínio geral. Disponibiliza variantes de diferentes dimensões (Standard, Mini, Nano), com **janela de contexto de até 400 000 tokens** e desempenho de **94,6% no AIME 2025** e **74,9% no SWE-bench Verified** [3][10]. O modelo apresenta **elevada consistência sintática** em tarefas de _function calling_, o que o torna particularmente eficaz na geração de JSON. Contudo, o custo médio de **≈ 1,25 USD/M input tokens** e a **latência média (~900 ms)** reportada em contextos dialogais limitam a sua adequação a sistemas em tempo real.

O **Gemini 2.5**, desenvolvido pela Google DeepMind, tem duas variantes principais — **Pro** (março de 2025) e **Flash** (setembro de 2025) [4][5]. O modo “Deep Think” reforça o raciocínio multimodal (**86,7% no AIME 2025**, **82% no MMMU**), enquanto o **Flash** otimiza latência e custo, reduzindo em cerca de **25% o número de tokens** gerados. Apesar do bom equilíbrio entre eficiência e qualidade, benchmarks independentes apontam **53,6% no SWE-bench Verified**, sugerindo desempenho moderado em tarefas estruturadas de código. A integração nativa no ecossistema Google constitui vantagem operacional, mas também uma restrição tecnológica.

O **Claude 4.5**, da Anthropic (setembro de 2025), destaca-se pela **segurança e explicabilidade**, registando **70,6% no SWE-bench Verified** e picos de **77,2%** em variantes otimizadas [8]. O custo elevado e a ausência de suporte _on-premises_ limitam a sua utilização em pipelines de inferência contínua, mas o modelo permanece uma referência de fiabilidade para aplicações empresariais com elevados requisitos de compliance.

---

### **Modelos Open-Source**

O **Llama 4**, lançado pela Meta a 5 de abril de 2025, combina uma arquitetura **Mixture-of-Experts (MoE)** com suporte multimodal [1][2]. Embora a Meta refira contextos de até **10 milhões de tokens**, não existe documentação técnica pública que valide este valor [1]; medições independentes reportam degradação de desempenho acima dos **256K tokens**. O modelo apresenta **21,0% no SWE-bench Verified**, refletindo limitações em tarefas estruturadas, mas destaca-se em aplicações que exigem **memória conversacional longa** e **custo reduzido**.

O **Qwen3**, da Alibaba Cloud (abril de 2025, atualizado em setembro), oferece variantes entre **0,6B e 235B parâmetros**, combinando subsistemas MoE e raciocínio simbólico [6]. Apresenta **81,5% no AIME 2025** e **69,6% no SWE-bench Verified**, com **modo adaptativo de raciocínio** que ajusta a profundidade segundo a complexidade da query. A possibilidade de **execução local** e o **baixo custo por token** tornam-no atrativo para experimentação híbrida, embora o desempenho varie consoante o idioma e a ambiguidade semântica.

O **DeepSeek V3**, modelo open-source de **235B parâmetros** (setembro de 2025) [9], é otimizado para raciocínio multilíngue e eficiência energética. Benchmarks independentes estimam **~80% no AIME** e **50–65% no SWE-bench**, mas a ausência de dados verificados sobre consistência multi-turn limita a sua avaliação no contexto NL→Dash.

O **Granite 3.2**, da IBM (fevereiro de 2025), centra-se em **governança de dados** e **conformidade empresarial**, sob licença **Apache 2.0** [7]. Apresenta baixo consumo e integração direta com **watsonx.ai** e **Ollama**, mas carece de resultados públicos em benchmarks como AIME ou SWE-bench, o que restringe a comparação direta com modelos de maior escala.

---

### **Tabela 3.1 — Comparação de Modelos LLM (Outubro 2025)**

|Modelo|Data Lanç.|Context Window|Latência (p50, ms)|Custo ($/M tokens)|AIME 2025 (%)|SWE-bench Verified (%)|Adequação ao NL→Dash|
|---|---|---|---|---|---|---|---|
|GPT-5|07/08/2025|400K|~900|1.25 (input)|94.6*|74.9*|Alta precisão e JSON consistente; custo elevado.|
|Gemini 2.5 Pro/Flash|03/2025–09/2025|1M+|~700 (Flash)|Médio|86.7*|53.6|Eficiência elevada; adequado a diálogos curtos.|
|Claude 4.5|09/2025|200K|~800|Alto|N/A|70.6 (pico 77.2*)|Forte em coding; custo e lock-in elevados.|
|Llama 4 Scout|05/04/2025|10M* (~256K prático)|~650|Baixo|N/A|21.0|Contexto longo; limitado em precisão estrutural.|
|Qwen3 235B/Max|04–09/2025|Variável|~750|Baixo (open)|81.5*|69.6|Bom custo/desempenho; tuning necessário.|
|DeepSeek V3|09/2025|128K|~800|Baixo|~80**|~60**|Multilingual eficiente; alta exigência de hardware.|
|Granite 3.2|26/02/2025|128K|~600|Baixo|N/A|N/A|Foco em enterprise e conformidade.|

(* self-reported; ** estimativas independentes — fontes [7–11])

---

## **Discussão Crítica e Adequação ao NL→Dash**

Nenhum modelo oferece simultaneamente a melhor precisão, menor latência e custo mais reduzido; o compromisso depende dos objetivos do sistema. Para aplicações **conversacionais em tempo real**, como o NL→Dash, priorizam-se **fidelidade estrutural**, **baixa latência (p95 < 1,2 s)** e **eficiência económica**.

O **Llama 4 Scout** e o **Gemini 2.5 Flash** apresentam a melhor relação custo-desempenho para protótipos, conciliando **contexto extenso**, **latência previsível** e **boa coerência de diálogo**. O **GPT-5** deve ser considerado como **referência de precisão** e **modelo de fallback** para intents ambíguos ou complexos. O **Qwen3** oferece um compromisso viável entre custo e desempenho, especialmente em cenários híbridos (cloud/local), enquanto o **Granite 3.2** é vantajoso para contextos empresariais com fortes restrições de dados. O **Claude 4.5** e o **DeepSeek V3** são modelos de nicho, adequados a casos específicos (coding e multilingual, respetivamente).

---

## **Síntese e Lacunas Identificadas**

A evolução recente dos LLMs revela convergência entre **eficiência arquitetural (MoE)** e **raciocínio seletivo**, mas persistem lacunas que afetam diretamente o domínio do NL→Dash:

1. Falta de mecanismos nativos de **validação e correção automática** de estruturas JSON;
    
2. Ausência de **benchmarks públicos** orientados à **fidelidade estrutural e consistência multi-turn**;
    
3. Dependência crescente de **ecossistemas proprietários**, que compromete a autonomia de experimentação e gestão de dados.
    

Consequentemente, o projeto-piloto deverá focar-se em modelos com **baixo custo por token**, **latência previsível** e **taxa de JSON válido ≥ 98%**, adotando o **Llama 4 Scout** (Cloudflare Workers AI) e o **Gemini 2.5 Flash** como bases experimentais. O **GPT-5 Mini** servirá de referência para calibração de qualidade.

Estas conclusões orientam a configuração do módulo de interpretação de intenções e o mecanismo de **refinamento conversacional** descritos no capítulo seguinte.

---

### **Referências**

[1] Meta AI, _The Llama 4 herd: The beginning of a new era..._, Meta Blog, abr. 2025.  
[2] Hugging Face, _Llama 4 Model Card_, 2025.  
[3] OpenAI, _Introducing GPT-5_, ago. 2025.  
[4] Google DeepMind, _Gemini 2.5: Deep Think and Flash Variants_, mar.–set. 2025.  
[5] Google AI Studio, _Gemini Pricing and Model Specs_, 2025.  
[6] Alibaba Cloud, _Qwen3 Model Card and Benchmarks_, abr.–set. 2025.  
[7] IBM Research, _Announcing Granite 3.2_, fev. 2025.  
[8] Anthropic, _Claude 4.5 Release Notes_, set. 2025.  
[9] DeepSeek AI, _DeepSeek V3 Technical Report_, set. 2025.  
[10] SWE-bench Leaderboard, _SWE-bench Verified_, out. 2025.  
[11] AIME 2025 Benchmark, _AIME 2025 Leaderboard_, out. 2025.  
[12] LiveCodeBench, _Leaderboard_, out. 2025.
