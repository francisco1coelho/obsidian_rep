[[State of art]]

# **Foundations and Related Technologies**

## **Introduction**

The first stage of the **NL→Dash** pipeline is responsible for interpreting the user’s initial request expressed in natural language. Its goal is to identify the **main intent** of the utterance (“create dashboard”, “update metric”, “apply filter”, “adjust threshold”) and to **extract minimal contextual information** (for example, the domain of the request or the type of metric mentioned), preparing the system for later stages of semantic mapping and structured generation.

## **Technological Analysis (LLMs with Public API)**

The execution of this phase depends on the ability of **Large Language Models (LLMs)** to understand intents, handle ambiguity, and maintain dialogue coherence. Selecting models with a **public API** is essential to enable rapid integration, controlled cost, and ease of experimentation.

The following section presents a critical analysis of the most relevant models, considering linguistic performance, inference latency, cost, context window size, and suitability to the project’s objectives.

### **Mistral 7B (Instruct)**

**Platform/API:** available via the **Hugging Face Inference API** and various _inference hosting_ services; supports _self-hosting_.  
**Advantages:** lightweight and fast model, ideal for **real-time** interactions; responds well to task-oriented _prompt engineering_; low cost and excellent performance in intent classification.  
**Limitations:** short context window, which limits the ability to retain conversational history; less semantic depth in ambiguous prompts.  
**Fit:** highly suitable for conversational prototypes and systems requiring **low latency** and **high throughput**.

### **Llama 3 (8B/70B)**

**Platform/API:** available on **Hugging Face**, **Together AI**, **Fireworks AI**, and other providers; supports _self-hosting_.  
**Advantages:** excellent _instruction-following_, strong contextual coherence, and high-quality text generation; extended context window and competitive latency in the 8B version.  
**Limitations:** the 70B variant is heavy for real-time operation; may require additional _decoding_ constraints to ensure consistent output formatting.  
**Fit:** ideal balance between performance and cost; particularly well-suited for **accurate intent interpretation** and **multi-turn dialogue**.

### **Falcon 7B/40B**

**Platform/API:** open-weight models supported by multiple providers and **Hugging Face Inference**.  
**Advantages:** stable and well-documented architecture; the 40B model provides solid linguistic coverage.  
**Limitations:** higher **inference latency** and less refined instruction-tuning compared to newer models; weaker dialogue quality without dedicated _fine-tuning_.  
**Fit:** acceptable in controlled scenarios but requires careful _prompt engineering_ to achieve consistency in intent detection tasks.

### **Gemma 2 (Google)**

**Platform/API:** available through **Google Cloud Inference API** and **Hugging Face**.  
**Advantages:** strong at interpreting user requests, high _instruction-following_ capability, and good contextual reasoning; suitable for _few-shot prompting_; enhanced safety features.  
**Limitations:** higher GPU usage and increased latency under heavy concurrency.  
**Fit:** strong ability for **semantic comprehension** and natural interpretation of _intents_, ideal for maintaining a stable conversational interface.

### **Mixtral (MoE 8×7B)**

**Platform/API:** available via **Hugging Face** and managed inference providers; requires specific infrastructure for _self-hosting_.  
**Advantages:** the _Mixture-of-Experts_ architecture combines **large-model quality** with **competitive latency**; highly effective for dialogue management and context coherence; good control over output format.  
**Limitations:** higher setup complexity and additional tuning required for inference stability.  
**Fit:** excellent for extended conversational interactions and compound requests where contextual coherence is essential.

---

## **Comparison and Fit within NL→Dash**

Performance in this stage must balance **inference speed** with **intent detection accuracy**.

- **Mistral 7B** and **Llama 3 8B** offer the best performance-to-cost ratio, making them ideal for **real-time systems** handling short, well-defined requests.
    
- **Gemma 2** and **Mixtral** excel in scenarios involving **continuous dialogue**, larger **context windows**, and **ambiguous language interpretation**.
    
- **Falcon 7B/40B**, while functional, shows lower consistency in natural-language intent generation without dedicated _fine-tuning_.
    

In the context of the NL→Dash pipeline—where the user may issue successive commands (“remove this metric”, “add a bar chart”) and the system must maintain context—**dialogue coherence and low latency** are key requirements. Therefore, **Llama 3 8B** or **Mistral 7B** represent ideal baseline options for initial deployment, with potential evolution toward **Mixtral** or **Gemma 2** as contextual dialogue complexity or semantic load increases.

---

## **Critical Synthesis**

This stage forms the interactive core of the **NL→Dash** system, where dialogue naturalness and intent accuracy determine the overall quality of the pipeline. Among the models evaluated, **Llama 3 (8B)** offers the best **technical trade-off** between cost, latency, and interpretive consistency, making it ideal for **conversational interfaces** and **real-time intent interpretation**.

**Mistral 7B** stands out as a lightweight, cost-effective alternative for low-latency operation, while **Gemma 2** and **Mixtral** are better suited for contexts requiring deeper contextual reasoning and extended dialogue handling.
