# **3.1 Fundamentos e Tecnologias Relacionadas**

A evolução recente dos **Modelos de Linguagem de Grande Escala (LLMs)** redefiniu a forma como sistemas computacionais interpretam e produzem linguagem natural. Estes modelos tornaram-se o núcleo de múltiplas soluções de inteligência artificial, permitindo transformar instruções textuais em representações estruturadas e executáveis. No contexto mais amplo dos sistemas de análise de dados, os LLMs representam a camada de **interpretação semântica** que viabiliza a tradução de comandos expressos em linguagem natural para formatos formais — como JSON, SQL ou especificações de visualização.

Assim, esta secção tem como objetivo **contextualizar o panorama tecnológico dos LLMs**, analisando a evolução arquitetónica, a diversidade de abordagens e os principais indicadores de desempenho. O propósito não é selecionar um modelo específico, mas sim **estabelecer uma base conceptual sólida** sobre as tecnologias atualmente disponíveis e os respetivos compromissos entre precisão, custo e eficiência — elementos essenciais para compreender o estado da arte dos sistemas de linguagem natural aplicados à geração estruturada de conteúdo.

---

## **Panorama Atual dos LLMs com API Pública**

O ecossistema de LLMs caracteriza-se por uma forte **diversificação de arquiteturas**, suportada por rápidas iterações de versões e um equilíbrio cada vez mais visível entre **modelos open-source** e **modelos proprietários**. As famílias **Llama 4**, **Qwen3** e **Granite 3.2** (de código aberto) e os modelos **GPT-5**, **Gemini 2.5** e **Claude 4.5** (comercialmente licenciados) representam os eixos principais de inovação.
As métricas comparativas aqui apresentadas foram recolhidas a partir de **model cards oficiais**, **leaderboards independentes** (SWE-bench Verified, AIME 2025, LiveCodeBench) e **documentação pública das APIs**, garantindo uma visão equilibrada entre dados auto-relatados e resultados verificados de forma independente.


---

### **Modelos Proprietários**

==O **GPT-5**, lançado pela OpenAI a 7 de agosto de 2025, representa o padrão de referência em raciocínio geral [3]. Disponibiliza variantes de diferentes dimensões (GPT-5, GPT-5 mini e GPT-5 nano) com uma **janela de contexto de até 400.000 tokens** (~272k _input_ + 128k _output_) [13].==
==O seu desempenho é demonstrado por **94,6% no AIME 2025**, indicativo de forte raciocínio matemático [3, 13], e **65,0% no SWE-bench Verified**, um _benchmark_ focado na resolução de problemas de engenharia de software [15]. Segundo a OpenAI, o modelo apresenta **elevada consistência sintática** em tarefas de _function calling_, o que o torna particularmente eficaz na geração de JSON [3].==
==Contudo, a sua adequação a sistemas com requisitos de tempo real é limitada pelo custo médio, de **≈ 1,25 USD por milhão de _input tokens_** [10], e por **tempos de resposta que variam entre ~1 e vários segundos**.==

==O **Gemini 2.5**, desenvolvido pela Google DeepMind, tem duas variantes principais: **Pro** (março de 2025) e **Flash** (setembro de 2025) [4]. O modo “Deep Think” do Pro reforça o raciocínio multimodal, com a Google a reportar **88% no AIME 2025** e **82% no MMMU** [4, 11]. Em contrapartida, o **Flash** otimiza latência e custo, reduzindo em cerca de **20% - 30% o número de tokens** gerados [5, 14]. Apesar do bom equilíbrio entre eficiência e qualidade, _benchmarks_ independentes de código posicionam o Gemini 2.5 Pro com **53,6% no SWE-bench Verified** [15], sugerindo desempenho moderado em tarefas estruturadas. A integração nativa no ecossistema Google constitui vantagem operacional, mas também uma restrição tecnológica.==

==O **Claude 4.5**, da Anthropic (setembro de 2025), destaca-se pela **segurança e explicabilidade**, liderando em tarefas de código com **70,6% no SWE-bench Verified** [15]. A Anthropic reporta ainda picos de **77,2%** em variantes otimizadas [8]. Contudo, o seu **custo elevado** — que a própria referência [15] corrobora, ao registar um custo médio de **$0.56 por tarefa** (o dobro dos $0.28 do GPT-5 na mesma métrica) — e a ausência de suporte _on-premises_ limitam a sua utilização em _pipelines_ de inferência contínua. O modelo permanece, assim, uma referência de fiabilidade para aplicações empresariais com elevados requisitos de _compliance_.==

---

### **Modelos Open-Source**

==O **Llama 4**, lançado pela Meta a 5 de abril de 2025, combina uma arquitetura **Mixture-of-Experts (MoE)** com suporte multimodal [1][2]. O modelo é fortemente focado em contexto longo, com a Meta a referir janelas de até **10 milhões de tokens** [1].==
==Em _benchmarks_ de precisão, contudo, o modelo revela limitações: apresenta **21,04% no SWE-bench Verified** [15], refletindo fraco desempenho em tarefas estruturadas. Notoriamente, o modelo **não possui resultados reportados no AIME 2025** [2][11], o que reforça a sua otimização para tarefas que não dependem de raciocínio complexo.==
==O seu custo de **$0.31 por tarefa** [15] é o seu principal atrativo. Conclui-se, assim, que o Llama 4 se destaca não como um concorrente de topo em precisão, mas como uma solução viável para aplicações que exigem **memória conversacional longa** a **baixo custo**.==

==O **Qwen3**, da Alibaba Cloud (abril de 2025, atualizado em setembro), oferece variantes entre **0,6B e 235B parâmetros**, combinando subsistemas MoE e raciocínio simbólico [6]. O modelo demonstra forte desempenho auto-relatado (_self-reported_), com **81,5% no AIME 2025** [6]. Em _benchmarks_ independentes de código, o seu desempenho é mais moderado: o _leaderboard_ **SWE-bench Verified [15]** regista o modelo Qwen3-Coder com **55,4%**.==
==A sua principal vantagem é o custo: sendo um modelo _open-source_, não está sujeito aos preços de API proprietária. O próprio _leaderboard_ [15] não atribui um custo por tarefa a esta submissão. A possibilidade de **execução local** (auto-hospedagem) ou o uso de APIs de terceiros resultam num **baixo custo por token**, o que o torna atrativo para experimentação híbrida.==

==O **Granite 3.2**, da IBM (fevereiro de 2025), segue uma estratégia distinta, centrada na **governança de dados e conformidade empresarial** [7]. Sob licença **Apache 2.0**, o modelo garante transparência e liberdade de uso comercial, com integração direta em ambientes _on-premises_ (Ollama e watsonx.ai). Embora careça de métricas públicas (AIME, SWE-bench) [11, 15], representa uma proposta sólida de **IA confiável e auditável** para indústrias reguladas.==

---

### **Tabela 3.1 — Comparação de Modelos LLM (Outubro 2025)**

| **Modelo**           | **Data Lanç.** | **Context Window**   | **Latência (p50, ms)** | **Custo ($/M tokens)** | **AIME 2025 (%)** | **SWE-bench Verified (%)** | **Adequação ao NL→Dash**                                  |
| -------------------- | -------------- | -------------------- | ---------------------- | ---------------------- | ----------------- | -------------------------- | --------------------------------------------------------- |
| **GPT-5**            | 07/08/2025     | 400K                 | ~900                   | 1.25 (input)           | 94.6*             | 65.0                       | Alta precisão e JSON consistente; custo elevado.          |
| **Gemini 2.5 Pro**   | 03/2025        | 1M+                  | ~950                   | Médio/Alto             | 86.7*             | 53.6                       | Desempenho de topo; alto custo e latência.                |
| **Claude 4.5**       | 09/2025        | 200K                 | ~800                   | Alto                   | N/A               | 70.6 (pico 77.2*)          | Forte em coding; custo e lock-in elevados.                |
| **Llama 4 Maverick** | 05/04/2025     | 10M* (~256K prático) | ~650                   | Baixo                  | N/A               | 21.04                      | Contexto longo; performance estrutural muito limitada.    |
| **Qwen3 235B/Max**   | 04–09/2025     | Variável             | ~750                   | Baixo (open)           | 81.5*             | 55.4                       | Bom custo/desempenho; tuning necessário.                  |
| **Granite 3.2**      | 26/02/2025     | 128K                 | ~600                   | Baixo                  | N/A               | N/A                        | Foco em enterprise e conformidade.                        |
| **Gemini 2.5 Flash** | 09/2025        | 1M+                  | ~700                   | Baixo                  | N/A               | 28.73                      | Eficiência elevada (custo/latência); adequado a diálogos. |

(* self-reported; ** estimativas independentes)

---
## **Síntese**

A análise comparativa do panorama de LLMs em outubro de 2025 evidencia uma clara **bifurcação no mercado**: por um lado, **modelos proprietários** que maximizam desempenho e precisão; por outro, **modelos open-source** otimizados para custo, eficiência e adaptabilidade. Esta distinção reflete duas estratégias tecnológicas complementares:  
(1) **a verticalização e integração total** de soluções empresariais com forte suporte e fiabilidade; e  
(2) **a descentralização do poder computacional**, impulsionada por modelos abertos com capacidade de execução local.
Em suma, os LLMs contemporâneos atingiram maturidade técnica e diversidade arquitetural suficientes para sustentar casos de uso complexos de interpretação e geração de dados estruturados.

---

### **Referências**

[1] Meta AI, _The Llama 4 herd: The beginning of a new era..._, Meta Blog, abr. 2025.  
[2] Hugging Face, _Llama 4 Model Card_, 2025.  
[3] OpenAI, “Introducing GPT-5,” Aug. 2025. [Online]. Available: https://openai.com/index/introducing-gpt-5/
[10] OpenAI, “API Pricing.” [Online]. Available: https://openai.com/api/pricing/
[13] OpenAI, “GPT-5,” Aug. 2025. [Online]. Available: https://openai.com/gpt-5/
[4] Google DeepMind, _Gemini 2.5: Deep Think and Flash Variants_, mar.–set. 2025.  https://deepmind.google/models/gemini/pro/
[14] https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/#flash-improvements
[15] https://www.swebench.com/?utm_source=chatgpt.com
[5] Google AI Studio, _Gemini Pricing and Model Specs_, 2025.  
[6] Alibaba Cloud, _Qwen3 Model Card and Benchmarks_, abr.–set. 2025.  
[7] IBM Research, _Announcing Granite 3.2_, fev. 2025.  
[8] Anthropic, _Claude 4.5 Release Notes_, set. 2025.  
[9] DeepSeek AI, _DeepSeek V3 Technical Report_, set. 2025.  
[11] AIME 2025 Benchmark, _AIME 2025 Leaderboard_, out. 2025.  
[12] LiveCodeBench, _Leaderboard_, out. 2025.
